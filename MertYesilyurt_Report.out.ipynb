{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1db5cb5",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [5]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f8a8a",
   "metadata": {
    "papermill": {
     "duration": 0.007038,
     "end_time": "2025-08-21T11:04:15.412525",
     "exception": false,
     "start_time": "2025-08-21T11:04:15.405487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How Do Lexical and Syntactic Features Affect Total Reading Time in Gaze‑Generated Texts?\n",
    "\n",
    "- **Name:** Mert Yeşilyurt\n",
    "- **Student ID:** 12999164"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240ae54",
   "metadata": {
    "papermill": {
     "duration": 0.005999,
     "end_time": "2025-08-21T11:04:15.424846",
     "exception": false,
     "start_time": "2025-08-21T11:04:15.418847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Requirements\n",
    "\n",
    "numpy==1.26.4\n",
    "\n",
    "pandas==2.3.1\n",
    "\n",
    "polars==1.31.0\n",
    "\n",
    "pyarrow==15.0.2\n",
    "\n",
    "matplotlib==3.10.3\n",
    "\n",
    "seaborn==0.13.2\n",
    "\n",
    "statsmodels==0.14.5\n",
    "\n",
    "scipy==1.13.1\n",
    "\n",
    "patsy==1.0.1\n",
    "\n",
    "spacy==3.8.0\n",
    "\n",
    "wordfreq==3.1.1\n",
    "\n",
    "pymovements==0.23.0\n",
    "\n",
    "requests>=2.28.0 \n",
    "\n",
    "ipython>=8.0.0\n",
    "\n",
    "openpyxl>=3.0.0\n",
    "\n",
    "xlrd>=2.0.1\n",
    "\n",
    "# spaCy English model (small)\n",
    "https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d1837",
   "metadata": {
    "papermill": {
     "duration": 0.005916,
     "end_time": "2025-08-21T11:04:15.436746",
     "exception": false,
     "start_time": "2025-08-21T11:04:15.430830",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The relationship between lexicon and syntax has long shaped the theories of language processing and psycholinguistics (Chomsky, 1957; Fodor, 1983). With advances in psycholinguistics and computational linguistics, eye-tracking experiments can now provide more insight into the effects of lexicon and syntax on language processing, specifically reading. Examples from eye-tracking studies show that reading-time measures are indeed affected by syntactical features such as dependency length and structural depth (Gibson, 2000; Grodner & Gibson, 2005). This is also the case for lexical frequency and word length as well, which is shown to have effects on reading measures through various studies that use total reading time (TRT) or variations to analyze linguistic features (Rayner, 1998; Kliegl et al., 2006). However, since reading experiments have repeated measures by subjects and items, studies that utilize TRT also employ mixed-effect models for generalizable estimates (Baayen et al., 2008). Inspired by such literature, this study analyzes how syntactic features (dependency length, structural depth) and lexical features (frequency, word length) affect reading using total reading time (TRT) and mixed-effects models. Unlike prior work, this study combines human eye-tracking with gaze‑generated texts to test whether the same feature-driven effects appear in gaze-generated texts.\n",
    "\n",
    "### Research question/hypothesis\n",
    "\n",
    "- RQ1: How do lexical (frequency, length) and syntactic features (dependency distance, syntactic depth) affect reading (total reading time/TRT) at the word level?\n",
    "- RQ2: Do gaze conditions successfully lead to differences in average TRT, and are feature distributions balanced across conditions?\n",
    "- H1: Higher frequency levels will lead to shorter total reading time, while on the other hand, longer words will lead to longer total reading time. (Rayner, 1998; Kliegl et al., 2006).\n",
    "- H2: Longer dependencies and greater depth will lead to longer total reading time (Gibson, 2000)\n",
    "- H3: The positive gaze condition will lead to features that increase the reading time, which will be associated with corresponding linguistic features, while the negative gaze condition will lead to a decrease in the reading time and will be accompanied by the corresponding linguistic features.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "**Dataset and Preprocessing:**\n",
    "- The GCTG dataset was used, excluding practice trials\n",
    "- Pymovements was used to detect fixations through IDT and map fixations to word-level AOIs\n",
    "- Total reading time (TRT) per word was calculated, including zeros for non-fixated words while excluding null values\n",
    "- Extreme TRTs (outside 150-4000ms range) were excluded\n",
    "- TRT was log-transformed for analysis\n",
    "\n",
    "**Linguistic Feature Extraction:**\n",
    "- Texts were tokenized and parsed with spaCy (en_core_web_sm)\n",
    "- Word length measured as alphabetic character count\n",
    "- Zipf frequency derived from SUBTLEX-US corpus with wordfreq fallback\n",
    "- Dependency distance measured as linear distance to syntactic head\n",
    "- Syntactic depth measured as distance from token to root\n",
    "- Integration cost calculated as a locality-based processing difficulty metric\n",
    "- AOI tokens were aligned to spaCy tokens using windowed matching\n",
    "- All continuous predictors were standardized (z-scored)\n",
    "\n",
    "**Statistical Analysis:**\n",
    "- Linear mixed-effects models (statsmodels MixedLM) predicting log(total reading time)\n",
    "- Fixed effects: standardized frequency, word length, dependency distance, syntactic depth, integration cost, and gaze condition (sum-coded)\n",
    "- Random effects: Model selection between (1) crossed random intercepts for subjects and items, or (2) subject slopes for frequency and length plus item variance components\n",
    "- Model selection based on AIC when possible, with REML estimation for final models\n",
    "- Marginal means estimated for each gaze condition with 95% confidence intervals\n",
    "- Results back-transformed to percent change in TRT for interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32058c",
   "metadata": {
    "papermill": {
     "duration": 0.005914,
     "end_time": "2025-08-21T11:04:15.449057",
     "exception": false,
     "start_time": "2025-08-21T11:04:15.443143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Implementation and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977796c7",
   "metadata": {
    "papermill": {
     "duration": 0.005882,
     "end_time": "2025-08-21T11:04:15.460878",
     "exception": false,
     "start_time": "2025-08-21T11:04:15.454996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Preprocessing for TRT\n",
    "\n",
    "This pipeline measures total reading time (TRT) per word AOI per subject using the gctg-clean dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa80adc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:04:15.473847Z",
     "iopub.status.busy": "2025-08-21T11:04:15.473629Z",
     "iopub.status.idle": "2025-08-21T11:04:16.284631Z",
     "shell.execute_reply": "2025-08-21T11:04:16.283982Z"
    },
    "papermill": {
     "duration": 0.818803,
     "end_time": "2025-08-21T11:04:16.285655",
     "exception": false,
     "start_time": "2025-08-21T11:04:15.466852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies for reproducibility\n",
    "%pip install -q pymovements polars matplotlib pyarrow statsmodels seaborn scipy patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95be2021",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:04:16.299243Z",
     "iopub.status.busy": "2025-08-21T11:04:16.299057Z",
     "iopub.status.idle": "2025-08-21T11:04:16.655266Z",
     "shell.execute_reply": "2025-08-21T11:04:16.654716Z"
    },
    "papermill": {
     "duration": 0.364189,
     "end_time": "2025-08-21T11:04:16.656401",
     "exception": false,
     "start_time": "2025-08-21T11:04:16.292212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBTLEX already present: data-clean/resources/SUBTLEX-US.csv\n"
     ]
    }
   ],
   "source": [
    "# Download SUBTLEX-US (60,384 words) into RESOURCES\n",
    "import io, zipfile, sys, subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Define locations locally \n",
    "BASE = Path(\"data-clean\")\n",
    "RESOURCES = BASE / \"resources\"\n",
    "\n",
    "RESOURCES.mkdir(parents=True, exist_ok=True)\n",
    "dest_csv = RESOURCES / 'SUBTLEX-US.csv'\n",
    "\n",
    "# Ensure engines for Excel\n",
    "def ensure_pkg(name):\n",
    "    try:\n",
    "        __import__(name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", name])\n",
    "            __import__(name)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "if dest_csv.exists():\n",
    "    print(f\"SUBTLEX already present: {dest_csv}\")\n",
    "else:\n",
    "    url = \"https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus/subtlexus4.zip\"\n",
    "    try:\n",
    "        print(f\"Downloading: {url}\")\n",
    "        r = requests.get(url, timeout=180)\n",
    "        r.raise_for_status()\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        # pick first Excel inside\n",
    "        member = next((n for n in z.namelist() if n.lower().endswith(('.xlsx', '.xls'))), None)\n",
    "        if member is None:\n",
    "            raise RuntimeError(\"No Excel file found in ZIP.\")\n",
    "        with z.open(member) as f:\n",
    "            data = f.read()\n",
    "        # pick engine based on extension\n",
    "        if member.lower().endswith('.xlsx'):\n",
    "            ok = ensure_pkg('openpyxl')\n",
    "            if not ok:\n",
    "                raise RuntimeError(\"openpyxl required to read .xlsx\")\n",
    "            df = pd.read_excel(io.BytesIO(data), engine='openpyxl')\n",
    "        else:\n",
    "            ok = ensure_pkg('xlrd')\n",
    "            if not ok:\n",
    "                raise RuntimeError(\"xlrd required to read .xls\")\n",
    "            df = pd.read_excel(io.BytesIO(data), engine='xlrd')\n",
    "        # normalize columns\n",
    "        lower_map = {c: str(c).strip().lower() for c in df.columns}\n",
    "        df = df.rename(columns=lower_map)\n",
    "        word_col = next((c for c in ('word', 'spelling', 'lemma', 'entry', 'item') if c in df.columns), None)\n",
    "        lg10_col = next((c for c in ('lg10wf', 'lg10_wf', 'lg10freq') if c in df.columns), None)\n",
    "        if word_col is None or lg10_col is None:\n",
    "            raise RuntimeError(f\"Expected 'Word' and 'Lg10WF' columns. Found: {list(df.columns)[:12]}\")\n",
    "        out_df = pd.DataFrame({\n",
    "            'form': df[word_col].astype(str).str.strip().str.lower(),\n",
    "            # Zipf (per billion) ≈ Lg10WF (per million) + 3\n",
    "            'zipf': pd.to_numeric(df[lg10_col], errors='coerce') + 3.0\n",
    "        }).dropna()\n",
    "        out_df.to_csv(dest_csv, index=False)\n",
    "        print(f\"Saved SUBTLEX to {dest_csv} | rows={len(out_df)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Warn] SUBTLEX download failed: {e}\")\n",
    "        print(\"Please download the 60,384-words ZIP (subtlexus4.zip) from the official page and extract to:\", dest_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae1709cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:04:16.670899Z",
     "iopub.status.busy": "2025-08-21T11:04:16.670659Z",
     "iopub.status.idle": "2025-08-21T11:04:16.747864Z",
     "shell.execute_reply": "2025-08-21T11:04:16.747284Z"
    },
    "papermill": {
     "duration": 0.085011,
     "end_time": "2025-08-21T11:04:16.748807",
     "exception": false,
     "start_time": "2025-08-21T11:04:16.663796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBTLEX rows: 20\n",
      "  form  zipf\n",
      "0   of   7.0\n",
      "1   is   6.7\n",
      "2  and   7.2\n",
      "3  the   7.5\n",
      "4  his   5.6\n"
     ]
    }
   ],
   "source": [
    "# Set SUBTLEX_PATH to the downloaded CSV and load it (comma-separated)\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "SUBTLEX_PATH = (RESOURCES / 'SUBTLEX-US.csv').resolve()\n",
    "assert SUBTLEX_PATH.exists(), f\"Missing SUBTLEX at {SUBTLEX_PATH}. Run the previous cell.\"\n",
    "\n",
    "def load_subtlex_csv(path: Path) -> pl.DataFrame:\n",
    "    df = pl.read_csv(path)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    if 'form' in cols and 'zipf' in cols:\n",
    "        return (\n",
    "            df.select([\n",
    "                pl.col(cols['form']).str.to_lowercase().alias('form'),\n",
    "                pl.col(cols['zipf']).cast(pl.Float64).alias('zipf'),\n",
    "            ])\n",
    "            .filter(pl.col('form').str.len_chars() > 0)\n",
    "            .filter(pl.col('zipf').is_not_null())\n",
    "            .group_by('form')\n",
    "            .agg(pl.col('zipf').max())\n",
    "        )\n",
    "    raise ValueError(f\"Unexpected SUBTLEX columns: {df.columns}\")\n",
    "\n",
    "subtlex = load_subtlex_csv(SUBTLEX_PATH)\n",
    "print('SUBTLEX rows:', subtlex.height)\n",
    "print(subtlex.head().to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad1c3179",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:04:16.762508Z",
     "iopub.status.busy": "2025-08-21T11:04:16.762255Z",
     "iopub.status.idle": "2025-08-21T11:04:18.807168Z",
     "shell.execute_reply": "2025-08-21T11:04:18.806710Z"
    },
    "papermill": {
     "duration": 2.052755,
     "end_time": "2025-08-21T11:04:18.808194",
     "exception": false,
     "start_time": "2025-08-21T11:04:16.755439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import polars as pl\n",
    "import pymovements as pm\n",
    "\n",
    "BASE = Path(\"data-clean\")\n",
    "PROCESSED_DIR = BASE / \"processed\"\n",
    "CACHE_DIR = PROCESSED_DIR / \"cache\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def is_practice(name: str) -> bool:\n",
    "    return \"practice\" in name.lower()\n",
    "\n",
    "\n",
    "def extract_condition(name: str) -> str:\n",
    "    lname = name.lower()\n",
    "    if \"neg\" in lname:\n",
    "        return \"neg\"\n",
    "    if \"pos\" in lname:\n",
    "        return \"pos\"\n",
    "    if \"zero\" in lname:\n",
    "        return \"zero\"\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b13aa",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a5c10b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T11:04:18.822054Z",
     "iopub.status.busy": "2025-08-21T11:04:18.821778Z",
     "iopub.status.idle": "2025-08-21T11:04:19.542533Z",
     "shell.execute_reply": "2025-08-21T11:04:19.541582Z"
    },
    "papermill": {
     "duration": 0.728425,
     "end_time": "2025-08-21T11:04:19.543383",
     "exception": true,
     "start_time": "2025-08-21T11:04:18.814958",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pymovements.dataset.dataset:        You are downloading the gctg dataset. Please be aware that pymovements does not\n",
      "        host or distribute any dataset resources and only provides a convenient interface to\n",
      "        download the public dataset resources that were published by their respective authors.\n",
      "\n",
      "        Please cite the referenced publication if you intend to use the dataset in your research.\n",
      "        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://syncandshare.lrz.de/dl/fiNmkHntgLXUC1jizcTYJ/gctg-data-clean.zip to data-clean/downloads/gctg-data-clean.zip\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "downloading resource https://syncandshare.lrz.de/dl/fiNmkHntgLXUC1jizcTYJ/gctg-data-clean.zip failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mgaierror\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:1344\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1336\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1335\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1382\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1381\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1331\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1091\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1094\u001b[39m \n\u001b[32m   1095\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1035\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1470\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1468\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mConnect to a host on a given (SSL) port.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1001\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1000\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:828\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    827\u001b[39m exceptions = []\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    829\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:963\u001b[39m, in \u001b[36mgetaddrinfo\u001b[39m\u001b[34m(host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    962\u001b[39m addrlist = []\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    964\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[31mgaierror\u001b[39m: [Errno -5] No address associated with hostname",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mURLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pymovements/dataset/dataset_download.py:194\u001b[39m, in \u001b[36m_download_resource_without_mirrors\u001b[39m\u001b[34m(resource, target_dirpath, verbose)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresource\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdirpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_dirpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# pylint: disable=overlapping-except\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pymovements/dataset/_utils/_downloads.py:87\u001b[39m, in \u001b[36mdownload_file\u001b[39m\u001b[34m(url, dirpath, filename, md5, max_redirect_hops, verbose)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# expand redirect chain if needed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m url = \u001b[43m_get_redirected_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_hops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_redirect_hops\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# download the file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pymovements/dataset/_utils/_downloads.py:140\u001b[39m, in \u001b[36m_get_redirected_url\u001b[39m\u001b[34m(url, max_hops)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_hops + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m    141\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m response.url == url \u001b[38;5;129;01mor\u001b[39;00m response.url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:515\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    514\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:532\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    531\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    491\u001b[39m func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:1392\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:1347\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[32m   1348\u001b[39m r = h.getresponse()\n",
      "\u001b[31mURLError\u001b[39m: <urlopen error [Errno -5] No address associated with hostname>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load clean dataset (as per lecturer's notebook, but using gctg-clean)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset = \u001b[43mpm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgctg-clean.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBASE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.load()\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Keep dataset as-is (no split); we'll handle stimulus grouping later\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded gaze frames: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset.gaze)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pymovements/dataset/dataset.py:1043\u001b[39m, in \u001b[36mDataset.download\u001b[39m\u001b[34m(self, extract, remove_finished, resume, verbose)\u001b[39m\n\u001b[32m   1002\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Download dataset resources.\u001b[39;00m\n\u001b[32m   1003\u001b[39m \n\u001b[32m   1004\u001b[39m \u001b[33;03mThis downloads all resources of the dataset. Per default this also extracts all archives\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1039\u001b[39m \u001b[33;03m    If downloading a resource failed for all given mirrors.\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1041\u001b[39m logger.info(\u001b[38;5;28mself\u001b[39m._disclaimer())\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m \u001b[43mdataset_download\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdefinition\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefinition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextract\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextract\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_finished\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_finished\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pymovements/dataset/dataset_download.py:94\u001b[39m, in \u001b[36mdownload_dataset\u001b[39m\u001b[34m(definition, paths, extract, remove_finished, resume, verbose)\u001b[39m\n\u001b[32m     91\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     92\u001b[39m             mirrors = definition.mirrors.get(content, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         \u001b[43m_download_resources\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmirrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresources\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefinition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtarget_dirpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownloads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extract:\n\u001b[32m    102\u001b[39m     extract_dataset(\n\u001b[32m    103\u001b[39m         definition=definition,\n\u001b[32m    104\u001b[39m         paths=paths,\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m         verbose=verbose,\n\u001b[32m    108\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pymovements/dataset/dataset_download.py:176\u001b[39m, in \u001b[36m_download_resources\u001b[39m\u001b[34m(mirrors, resources, target_dirpath, verbose)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m resource \u001b[38;5;129;01min\u001b[39;00m resources:\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mirrors:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m         \u001b[43m_download_resource_without_mirrors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dirpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m mirrors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pymovements/dataset/dataset_download.py:204\u001b[39m, in \u001b[36m_download_resource_without_mirrors\u001b[39m\u001b[34m(resource, target_dirpath, verbose)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# pylint: disable=overlapping-except\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (URLError, \u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    205\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdownloading resource \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m failed.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    206\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01merror\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: downloading resource https://syncandshare.lrz.de/dl/fiNmkHntgLXUC1jizcTYJ/gctg-data-clean.zip failed."
     ]
    }
   ],
   "source": [
    "# Load clean dataset (as per lecturer's notebook, but using gctg-clean)\n",
    "dataset = pm.Dataset(\"gctg-clean.yaml\", str(BASE)).download().load()\n",
    "# Keep dataset as-is (no split); we'll handle stimulus grouping later\n",
    "print(f\"Loaded gaze frames: {len(dataset.gaze)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3f4eb2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect stimuli and exclude practice\n",
    "all_samples = pl.concat([g.samples for g in dataset.gaze])\n",
    "stimulus_names = all_samples[\"stimulus\"].unique().to_list()\n",
    "stimulus_names = [s for s in stimulus_names if not is_practice(s)]\n",
    "subjects = all_samples[\"subject_id\"].unique().to_list()\n",
    "print(f\"Stimuli (non-practice): {len(stimulus_names)} | Subjects: {len(subjects)} -> {sorted(subjects)}\")\n",
    "stimulus_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b09ad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load word AOIs for each stimulus from the clean package\n",
    "stimuli = {}\n",
    "missing_aois = []\n",
    "for stimulus_name in stimulus_names:\n",
    "    aois_path = BASE / \"raw\" / \"stimuli\" / f\"{stimulus_name}.word.csv\"\n",
    "    if not aois_path.exists():\n",
    "        missing_aois.append(stimulus_name)\n",
    "        continue\n",
    "    aois = pl.read_csv(aois_path)\n",
    "    stimulus = pm.stimulus.TextStimulus(\n",
    "        aois,\n",
    "        aoi_column=\"content\",\n",
    "        start_x_column=\"left\",\n",
    "        start_y_column=\"top\",\n",
    "        end_x_column=\"right\",\n",
    "        end_y_column=\"bottom\",\n",
    "    )\n",
    "    stimuli[stimulus_name] = stimulus\n",
    "print(f\"Loaded AOIs for {len(stimuli)} stimuli. Missing AOIs: {len(missing_aois)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf46e2d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Event detection using IDT defaults, then fixation locations in pixel space\n",
    "# (matches lecturer’s pipeline: pix2deg -> detect(\"idt\") -> compute_properties(\"location\", pixel))\n",
    "dataset.pix2deg()\n",
    "dataset.detect(\"idt\", clear=True)\n",
    "dataset.compute_properties((\"location\", {\"position_column\": \"pixel\"}))\n",
    "print(\"Events detected and fixation locations computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2509f3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deprecated mapping approach (kept as no-op to maintain cell order)\n",
    "# Mapping to AOIs is handled later via explicit per-subject-per-stimulus processing.\n",
    "len([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d06ff",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick head (does not modify outputs)\n",
    "trt.sample(n=min(5, trt.height)) if 'trt' in locals() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c1dc9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map fixations to word AOIs via point-in-rectangle and compute TRT reliably\n",
    "\n",
    "def map_events_to_aois(events_df: pl.DataFrame, aoi_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # events_df has 'location' as [x, y] list; expand to columns\n",
    "    ev = events_df.with_columns([\n",
    "        pl.col(\"location\").list.first().alias(\"x\"),\n",
    "        pl.col(\"location\").list.last().alias(\"y\"),\n",
    "    ])\n",
    "    # Ensure AOI columns exist\n",
    "    aois = aoi_df.select([\n",
    "        pl.col(\"index\"),\n",
    "        pl.col(\"content\"),\n",
    "        pl.col(\"left\"),\n",
    "        pl.col(\"right\"),\n",
    "        pl.col(\"top\"),\n",
    "        pl.col(\"bottom\"),\n",
    "    ])\n",
    "    # Cross-join and filter by within-rect; for performance we can prefilter by x/y bounds\n",
    "    ev_small = ev.select([\"subject_id\", \"stimulus\", \"duration\", \"x\", \"y\"])  # keep necessary cols\n",
    "    joined = ev_small.join(aois, how=\"cross\")\n",
    "    mapped = joined.filter(\n",
    "        (pl.col(\"x\") >= pl.col(\"left\")) & (pl.col(\"x\") <= pl.col(\"right\")) &\n",
    "        (pl.col(\"y\") >= pl.col(\"top\")) & (pl.col(\"y\") <= pl.col(\"bottom\"))\n",
    "    )\n",
    "    # Aggregate TRT by AOI\n",
    "    trt = (\n",
    "        mapped\n",
    "        .group_by([\"subject_id\", \"stimulus\", \"index\", \"content\"]) \n",
    "        .agg(pl.col(\"duration\").sum().alias(\"total_reading_time\"))\n",
    "    )\n",
    "    # Right join to include AOIs with TRT=0\n",
    "    # Use stimulus-wide AOIs; fill subject_id/stimulus later\n",
    "    trt_full = (\n",
    "        trt.join(aois.select([\"index\", \"content\"]), on=[\"index\", \"content\"], how=\"right\")\n",
    "        .with_columns(pl.col(\"total_reading_time\").fill_null(0))\n",
    "    )\n",
    "    # Fill subject and stimulus ids (unique within events)\n",
    "    subj = events_df[\"subject_id\"].unique().item()\n",
    "    stim = events_df[\"stimulus\"].unique().item()\n",
    "    trt_full = trt_full.with_columns([\n",
    "        pl.lit(subj).alias(\"subject_id\").cast(pl.Utf8),\n",
    "        pl.lit(stim).alias(\"stimulus\").cast(pl.Utf8),\n",
    "        pl.lit(extract_condition(stim)).alias(\"condition\")\n",
    "    ])\n",
    "    return trt_full.select([\"subject_id\", \"stimulus\", \"condition\", \"index\", \"content\", \"total_reading_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890338c3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Corrected processing: iterate per subject and per stimulus within subject\n",
    "trt_tables = []\n",
    "cache_written = 0\n",
    "for g in dataset.gaze:\n",
    "    ev_all = g.events.frame\n",
    "    subj = ev_all[\"subject_id\"].unique().item()\n",
    "    stims = ev_all[\"stimulus\"].unique().to_list()\n",
    "    for stim in stims:\n",
    "        if is_practice(stim):\n",
    "            continue\n",
    "        if stim not in stimuli:\n",
    "            continue\n",
    "        sub_ev = ev_all.filter(pl.col(\"stimulus\") == stim)\n",
    "        # cache raw events with locations\n",
    "        out_path = CACHE_DIR / f\"events_{subj}_{stim}.parquet\"\n",
    "        sub_ev.write_parquet(out_path)\n",
    "        cache_written += 1\n",
    "        # compute TRT\n",
    "        trt_tables.append(map_events_to_aois(sub_ev, stimuli[stim].aois))\n",
    "\n",
    "print(f\"Cached event tables: {cache_written}\")\n",
    "if trt_tables:\n",
    "    trt = pl.concat(trt_tables, how=\"vertical_relaxed\")\n",
    "    out_csv = PROCESSED_DIR / \"trt_by_word.csv\"\n",
    "    out_parquet = PROCESSED_DIR / \"trt_by_word.parquet\"\n",
    "    trt.write_csv(out_csv)\n",
    "    trt.write_parquet(out_parquet)\n",
    "    print(f\"Saved TRT rows: {trt.height} -> {out_csv}\")\n",
    "else:\n",
    "    raise RuntimeError(\"No TRT tables produced. Verify events and AOIs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d82348",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preview a small sample of the final TRT table (non-destructive)\n",
    "trt.head(10).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a6b2da",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Linguistic Features Pipeline\n",
    "\n",
    "This pipeline extracts the following linguistic features:\n",
    "- **Word frequency**: SUBTLEX-US Zipf scores (lemma-first, then surface)\n",
    "- **Word length**: Alphabetic character count\n",
    "- **Dependency distance**: Linear distance to syntactic head (Gibson 2000 locality theory)\n",
    "- **Syntactic tree depth**: Distance from token to root\n",
    "- **Integration cost**: Locality-based processing difficulty metric\n",
    "\n",
    "Features are aligned to AOI tokens and merged with TRT data for mixed-effects modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c10a92",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install dependencies with version pins for reproducibility\n",
    "%pip install -q \"spacy==3.8.0\" \"polars==1.31.0\" \"pyarrow==15.0.2\" \"wordfreq==3.1.1\"\n",
    "\n",
    "# Download spaCy model if not present\n",
    "import subprocess\n",
    "import sys\n",
    "try:\n",
    "    import en_core_web_sm\n",
    "except ImportError:\n",
    "    print(\"Downloading spaCy English model...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253c83a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import polars as pl\n",
    "import spacy\n",
    "\n",
    "# Paths\n",
    "BASE = Path(\"data-clean\")\n",
    "RAW = BASE / \"raw\"\n",
    "TEXTS_DIR = RAW / \"texts\"\n",
    "STIMULI_DIR = RAW / \"stimuli\"\n",
    "PROC = BASE / \"processed\"\n",
    "RESOURCES = BASE / \"resources\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "TRT_PARQUET = PROC / \"trt_by_word.parquet\"\n",
    "TRT_CSV = PROC / \"trt_by_word.csv\"\n",
    "SUBTLEX_PATH = RESOURCES / \"SUBTLEX-US.csv\"\n",
    "\n",
    "print(f\"Base directory: {BASE.absolute()}\")\n",
    "print(f\"SUBTLEX exists: {SUBTLEX_PATH.exists()}\")\n",
    "print(f\"TRT data exists: {TRT_PARQUET.exists() or TRT_CSV.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7bce7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def is_practice(name: str) -> bool:\n",
    "    return \"practice\" in name.lower()\n",
    "\n",
    "def extract_condition(name: str) -> str:\n",
    "    lname = name.lower()\n",
    "    if \"neg\" in lname: return \"neg\"\n",
    "    if \"pos\" in lname: return \"pos\"  \n",
    "    if \"zero\" in lname: return \"zero\"\n",
    "    return \"unknown\"\n",
    "\n",
    "# Text normalization for alignment: keep apostrophes & hyphens\n",
    "_punct_re = re.compile(r\"[^\\w'\\-]+\", flags=re.UNICODE)\n",
    "\n",
    "def normalize_token(s: str) -> str:\n",
    "    \"\"\"Normalize token for alignment: lowercase; keep apostrophes & hyphens; strip other punct.\"\"\"\n",
    "    return _punct_re.sub(\"\", s.lower())\n",
    "\n",
    "def align_aoi_to_spacy_windowed(aoi_tokens: list[str], doc_tokens: list[str], max_window: int = 2) -> list[int | None]:\n",
    "    \"\"\"\n",
    "    Greedy left-to-right alignment by normalized surface forms.\n",
    "    Supports concatenating up to `max_window` spaCy tokens to match hyphenated/multiword AOIs.\n",
    "    Also aligns punctuation-only AOIs to identical doc tokens.\n",
    "    \"\"\"\n",
    "    mapping: list[int | None] = [None] * len(aoi_tokens)\n",
    "    j = 0\n",
    "    N = len(doc_tokens)\n",
    "\n",
    "    for i, aoi_tok in enumerate(aoi_tokens):\n",
    "        raw = aoi_tok.strip()\n",
    "        tgt = normalize_token(aoi_tok)\n",
    "\n",
    "        # Handle pure punctuation AOIs by literal match\n",
    "        if tgt == \"\" and raw:\n",
    "            while j < N and doc_tokens[j].strip() != raw:\n",
    "                j += 1\n",
    "            if j < N and doc_tokens[j].strip() == raw:\n",
    "                mapping[i] = j\n",
    "                j += 1\n",
    "            continue\n",
    "\n",
    "        if tgt == \"\":\n",
    "            # empty after normalization; skip\n",
    "            continue\n",
    "\n",
    "        matched = False\n",
    "        k = j\n",
    "        while k < N and not matched:\n",
    "            for w in range(1, max_window + 1):\n",
    "                if k + w > N:\n",
    "                    break\n",
    "                window_norm = \"\".join(normalize_token(t) for t in doc_tokens[k:k + w])\n",
    "                if window_norm == tgt:\n",
    "                    mapping[i] = k\n",
    "                    j = k + w\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                k += 1\n",
    "\n",
    "        if not matched:\n",
    "            j = min(j + 1, N)\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856ea73",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load TRT data to get stimulus list (excludes practice)\n",
    "if TRT_PARQUET.exists():\n",
    "    trt = pl.read_parquet(TRT_PARQUET)\n",
    "else:\n",
    "    trt = pl.read_csv(TRT_CSV)\n",
    "\n",
    "# Filter out practice stimuli\n",
    "trt = trt.filter(~pl.col(\"stimulus\").str.contains(\"practice\"))\n",
    "stimuli_to_process = trt[\"stimulus\"].unique().to_list()\n",
    "\n",
    "print(f\"Stimuli to process (from TRT): {len(stimuli_to_process)}\")\n",
    "print(f\"Subjects: {trt['subject_id'].n_unique()}\")\n",
    "print(f\"Conditions: {sorted(trt['condition'].unique().to_list())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172df30",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load AOI data for alignment\n",
    "def load_aois(stimulus: str) -> pl.DataFrame | None:\n",
    "    \"\"\"Load AOI word-level data for a stimulus.\"\"\"\n",
    "    aoi_path = STIMULI_DIR / f\"{stimulus}.word.csv\"\n",
    "    if not aoi_path.exists():\n",
    "        return None\n",
    "        \n",
    "    aoi_df = pl.read_csv(aoi_path)\n",
    "    required_cols = {\"index\", \"content\", \"left\", \"right\", \"top\", \"bottom\"}\n",
    "    missing_cols = required_cols - set(aoi_df.columns)\n",
    "    \n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"AOI file missing columns {missing_cols} for {stimulus}\")\n",
    "        \n",
    "    return aoi_df.sort(\"index\")  # Ensure consistent token order\n",
    "\n",
    "# Load AOIs for all stimuli\n",
    "stimulus_aois: dict[str, pl.DataFrame] = {}\n",
    "missing_aois = []\n",
    "\n",
    "for stimulus in stimuli_to_process:\n",
    "    if is_practice(stimulus):\n",
    "        continue\n",
    "        \n",
    "    aoi_df = load_aois(stimulus)\n",
    "    if aoi_df is not None:\n",
    "        stimulus_aois[stimulus] = aoi_df\n",
    "    else:\n",
    "        missing_aois.append(stimulus)\n",
    "\n",
    "print(f\"Loaded AOIs: {len(stimulus_aois)} stimuli\")\n",
    "if missing_aois:\n",
    "    print(f\"Missing AOI files: {missing_aois}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f50ebd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text loading with fallbacks\n",
    "def load_text(stimulus: str, aoi_df: pl.DataFrame) -> str:\n",
    "    \"\"\"Load stimulus text from JSON, TXT, or reconstruct from AOIs.\"\"\"\n",
    "    # Try JSON first\n",
    "    json_path = TEXTS_DIR / f\"{stimulus}.json\"\n",
    "    if json_path.exists():\n",
    "        try:\n",
    "            data = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
    "            # Handle common JSON structures\n",
    "            if isinstance(data, dict):\n",
    "                for key in (\"text\", \"content\", \"body\"):\n",
    "                    if key in data and isinstance(data[key], str) and data[key].strip():\n",
    "                        return data[key]\n",
    "                # Fallback: first string value\n",
    "                for value in data.values():\n",
    "                    if isinstance(value, str) and value.strip():\n",
    "                        return value\n",
    "        except (json.JSONDecodeError, UnicodeDecodeError):\n",
    "            pass\n",
    "    \n",
    "    # Try TXT file\n",
    "    txt_path = TEXTS_DIR / f\"{stimulus}.txt\" \n",
    "    if txt_path.exists():\n",
    "        try:\n",
    "            return txt_path.read_text(encoding=\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Fallback: reconstruct from AOI content\n",
    "    return \" \".join(aoi_df[\"content\"].to_list())\n",
    "\n",
    "# Test text loading for first few stimuli\n",
    "for i, (stimulus, aoi_df) in enumerate(list(stimulus_aois.items())[:3]):\n",
    "    text = load_text(stimulus, aoi_df)\n",
    "    print(f\"{stimulus}: {len(text)} chars, first 100: {text[:100]!r}\")\n",
    "    if i >= 2:  # Only show first 3\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee5794",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize spaCy pipeline\n",
    "print(\"Loading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])  # Exclude NER for speed\n",
    "\n",
    "# Ensure sentence segmentation\n",
    "if not nlp.has_pipe(\"senter\") and not nlp.has_pipe(\"parser\"):\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "print(f\"spaCy pipeline: {nlp.pipe_names}\")\n",
    "\n",
    "# Test parsing\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "doc = nlp(test_text)\n",
    "print(f\"Test parse: {len(doc)} tokens, {len(list(doc.sents))} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769ade6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Robust SUBTLEX loader (pandas -> polars)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "BASE = Path(\"data-clean\")\n",
    "RESOURCES = BASE / \"resources\"\n",
    "SUBTLEX_PATH = RESOURCES / \"SUBTLEX-US.csv\"\n",
    "\n",
    "print(\"Loading SUBTLEX-US...\")\n",
    "if not SUBTLEX_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing {SUBTLEX_PATH}. Run the downloader cell above.\")\n",
    "\n",
    "# Read with pandas to avoid dtype pitfalls, then convert to polars\n",
    "df_pd = pd.read_csv(SUBTLEX_PATH)\n",
    "cols_lower = {c: str(c).strip().lower() for c in df_pd.columns}\n",
    "df_pd = df_pd.rename(columns=cols_lower)\n",
    "\n",
    "# Harmonize columns\n",
    "if 'form' not in df_pd.columns:\n",
    "    cand = next((c for c in df_pd.columns if c in ('word','spelling','lemma','entry','item')), None)\n",
    "    if cand:\n",
    "        df_pd = df_pd.rename(columns={cand:'form'})\n",
    "if 'zipf' not in df_pd.columns:\n",
    "    cand = next((c for c in df_pd.columns if c.lower() in ('zipf','lg10wf','lg10_wf')), None)\n",
    "    if cand:\n",
    "        if cand.lower().startswith('lg10'):\n",
    "            df_pd['zipf'] = pd.to_numeric(df_pd[cand], errors='coerce') + 3.0\n",
    "        else:\n",
    "            df_pd['zipf'] = pd.to_numeric(df_pd[cand], errors='coerce')\n",
    "\n",
    "# Clean\n",
    "df_pd['form'] = df_pd['form'].astype(str).str.strip().str.lower()\n",
    "df_pd['zipf'] = pd.to_numeric(df_pd['zipf'], errors='coerce')\n",
    "df_pd = df_pd.dropna(subset=['form','zipf'])\n",
    "\n",
    "# Deduplicate by maximum zipf\n",
    "subtlex = pl.from_pandas(\n",
    "    df_pd.sort_values(['form','zipf']).drop_duplicates('form', keep='last')\n",
    ")[['form','zipf']]\n",
    "\n",
    "print(f\"SUBTLEX entries: {subtlex.height}\")\n",
    "print(subtlex.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3002d7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dependency Locality Theory functions\n",
    "\n",
    "def token_depth(token) -> int:\n",
    "    \"\"\"Calculate syntactic tree depth: distance from token to root.\"\"\"\n",
    "    depth = 0\n",
    "    current = token\n",
    "    while current.head != current:  # Until we reach root\n",
    "        depth += 1\n",
    "        current = current.head\n",
    "        if depth > 50:  # Prevent infinite loops\n",
    "            break\n",
    "    return depth\n",
    "\n",
    "def dependency_distance(token) -> int:\n",
    "    \"\"\"Calculate linear dependency distance: |position - head_position|.\"\"\"\n",
    "    return abs(token.i - token.head.i)\n",
    "\n",
    "def integration_cost(token) -> float:\n",
    "    \"\"\"\n",
    "    Calculate integration cost based on Gibson 2000 Dependency Locality Theory.\n",
    "    \n",
    "    Integration cost reflects processing difficulty due to:\n",
    "    1. Linear distance to syntactic head\n",
    "    2. Number of discourse referents between dependent and head\n",
    "    \n",
    "    Simplified metric: linear distance weighted by dependency type.\n",
    "    \"\"\"\n",
    "    if token.head == token:  # Root has no integration cost\n",
    "        return 0.0\n",
    "    \n",
    "    dist = dependency_distance(token)\n",
    "    \n",
    "    # Weight by dependency relation importance (simplified)\n",
    "    # More important relations have higher integration costs\n",
    "    relation_weights = {\n",
    "        \"nsubj\": 1.0,     # Subject\n",
    "        \"dobj\": 1.0,      # Direct object  \n",
    "        \"prep\": 0.8,      # Prepositional\n",
    "        \"amod\": 0.6,      # Adjectival modifier\n",
    "        \"advmod\": 0.6,    # Adverbial modifier\n",
    "        \"det\": 0.3,       # Determiner\n",
    "        \"aux\": 0.3,       # Auxiliary\n",
    "        \"punct\": 0.1,     # Punctuation\n",
    "    }\n",
    "    \n",
    "    weight = relation_weights.get(token.dep_, 0.5)  # Default weight\n",
    "    \n",
    "    # Integration cost = distance * relation_weight\n",
    "    # Add small penalty for very long distances\n",
    "    cost = dist * weight\n",
    "    if dist > 5:\n",
    "        cost += (dist - 5) * 0.1  # Additional penalty for long dependencies\n",
    "        \n",
    "    return cost\n",
    "\n",
    "def locality_features(token) -> dict:\n",
    "    \"\"\"Extract all locality-related features for a token.\"\"\"\n",
    "    return {\n",
    "        \"dep_dist\": dependency_distance(token),\n",
    "        \"depth\": token_depth(token),\n",
    "        \"integration_cost\": integration_cost(token),\n",
    "        \"dep_label\": token.dep_,\n",
    "        \"pos_tag\": token.pos_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df0198",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Frequency lookup function\n",
    "from wordfreq import zipf_frequency\n",
    "\n",
    "def lookup_zipf(surface: str, lemma: str = \"\") -> float | None:\n",
    "    \"\"\"\n",
    "    Look up Zipf frequency score.\n",
    "    SUBTLEX-first, then wordfreq fallback. Candidates include lemma/surface,\n",
    "    strip possessive 's, de-hyphenized form, and hyphen parts.\n",
    "    \"\"\"\n",
    "    candidates: list[str] = []\n",
    "    if lemma:\n",
    "        candidates.append(lemma)\n",
    "    if surface:\n",
    "        candidates.append(surface)\n",
    "        low = surface.lower()\n",
    "        # strip possessive\n",
    "        if low.endswith((\"’s\", \"'s\")) and len(surface) > 2:\n",
    "            candidates.append(surface[:-2])\n",
    "        # de-hyphenated and parts\n",
    "        if \"-\" in surface:\n",
    "            candidates.append(surface.replace(\"-\", \"\"))\n",
    "            candidates.extend([p for p in surface.split(\"-\") if p])\n",
    "\n",
    "    # Normalize and deduplicate\n",
    "    norm = []\n",
    "    seen = set()\n",
    "    for c in candidates:\n",
    "        n = normalize_token(c)\n",
    "        if n and n not in seen:\n",
    "            seen.add(n)\n",
    "            norm.append(n)\n",
    "\n",
    "    # 1) SUBTLEX lookup\n",
    "    for n in norm:\n",
    "        hit = subtlex.filter(pl.col(\"form\") == n).select(\"zipf\")\n",
    "        if hit.height:\n",
    "            return float(hit.item())\n",
    "\n",
    "    # 2) wordfreq fallback\n",
    "    for n in norm:\n",
    "        z = zipf_frequency(n, \"en\")\n",
    "        if z > 0:\n",
    "            return float(z)\n",
    "\n",
    "    return None  # Not found\n",
    "\n",
    "# Test frequency lookup\n",
    "test_words = [\"the\", \"cat\", \"quickly\", \"xyzabc\"]\n",
    "for word in test_words:\n",
    "    freq = lookup_zipf(word)\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912cd87",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main feature extraction\n",
    "print(\"Extracting linguistic features...\")\n",
    "\n",
    "feature_tables = []\n",
    "coverage_stats = []\n",
    "\n",
    "for stimulus_idx, (stimulus, aoi_df) in enumerate(stimulus_aois.items()):\n",
    "    print(f\"Processing {stimulus_idx+1}/{len(stimulus_aois)}: {stimulus}\")\n",
    "    \n",
    "    # Load and parse text\n",
    "    text = load_text(stimulus, aoi_df)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Get non-space tokens for alignment\n",
    "    doc_tokens_all = [t for t in doc if not t.is_space]\n",
    "    doc_tokens_text = [t.text for t in doc_tokens_all]\n",
    "    \n",
    "    # Get AOI tokens in order\n",
    "    aoi_tokens = aoi_df.sort(\"index\")[\"content\"].to_list()\n",
    "    aoi_indices = aoi_df.sort(\"index\")[\"index\"].to_list()\n",
    "    \n",
    "    # Align AOI tokens to spaCy tokens (windowed)\n",
    "    alignment = align_aoi_to_spacy_windowed(aoi_tokens, doc_tokens_text, max_window=2)\n",
    "    \n",
    "    # Calculate coverage\n",
    "    aligned_count = sum(1 for x in alignment if x is not None)\n",
    "    coverage = aligned_count / max(1, len(aoi_tokens))\n",
    "    coverage_stats.append((stimulus, coverage, len(aoi_tokens), aligned_count))\n",
    "    \n",
    "    # Extract features for each AOI token\n",
    "    rows = []\n",
    "    for aoi_idx, content, spacy_idx in zip(aoi_indices, aoi_tokens, alignment):\n",
    "        if spacy_idx is None:\n",
    "            # Unaligned token - compute basic features only\n",
    "            rows.append({\n",
    "                \"stimulus\": stimulus,\n",
    "                \"index\": int(aoi_idx),\n",
    "                \"content\": content,\n",
    "                \"word_len\": sum(c.isalpha() for c in content),\n",
    "                \"freq_zipf\": None,\n",
    "                \"dep_dist\": None,\n",
    "                \"depth\": None,\n",
    "                \"integration_cost\": None,\n",
    "                \"dep_label\": None,\n",
    "                \"pos_tag\": None,\n",
    "                \"lemma\": None,\n",
    "                \"sentence_id\": None,\n",
    "                \"token_id_sent\": None,\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Get spaCy token\n",
    "        token = doc_tokens_all[spacy_idx]\n",
    "        \n",
    "        # Calculate features\n",
    "        word_len = sum(c.isalpha() for c in content)\n",
    "        lemma = token.lemma_ if hasattr(token, 'lemma_') else \"\"\n",
    "        freq_zipf = lookup_zipf(content, lemma)\n",
    "        \n",
    "        # Locality features  \n",
    "        locality = locality_features(token)\n",
    "        \n",
    "        # Sentence information\n",
    "        sentence_id = None\n",
    "        token_id_sent = None\n",
    "        for sent_idx, sent in enumerate(doc.sents):\n",
    "            if token.i >= sent.start and token.i < sent.end:\n",
    "                sentence_id = sent_idx\n",
    "                token_id_sent = token.i - sent.start\n",
    "                break\n",
    "        \n",
    "        rows.append({\n",
    "            \"stimulus\": stimulus,\n",
    "            \"index\": int(aoi_idx),\n",
    "            \"content\": content,\n",
    "            \"word_len\": word_len,\n",
    "            \"freq_zipf\": freq_zipf,\n",
    "            \"dep_dist\": locality[\"dep_dist\"],\n",
    "            \"depth\": locality[\"depth\"], \n",
    "            \"integration_cost\": locality[\"integration_cost\"],\n",
    "            \"dep_label\": locality[\"dep_label\"],\n",
    "            \"pos_tag\": locality[\"pos_tag\"],\n",
    "            \"lemma\": lemma,\n",
    "            \"sentence_id\": sentence_id,\n",
    "            \"token_id_sent\": token_id_sent,\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    stimulus_features = pl.from_records(rows)\n",
    "    feature_tables.append(stimulus_features)\n",
    "\n",
    "print(\"Feature extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb4fc7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Consolidate all features\n",
    "features = pl.concat(feature_tables, how=\"vertical_relaxed\")\n",
    "\n",
    "# Cast to appropriate types\n",
    "features = features.with_columns([\n",
    "    pl.col(\"stimulus\").cast(pl.Utf8),\n",
    "    pl.col(\"index\").cast(pl.Int64),  # Match TRT data type\n",
    "    pl.col(\"content\").cast(pl.Utf8),\n",
    "    pl.col(\"word_len\").cast(pl.Int16),\n",
    "    pl.col(\"freq_zipf\").cast(pl.Float64),\n",
    "    pl.col(\"dep_dist\").cast(pl.Int16),\n",
    "    pl.col(\"depth\").cast(pl.Int16),\n",
    "    pl.col(\"integration_cost\").cast(pl.Float64),\n",
    "    pl.col(\"dep_label\").cast(pl.Utf8),\n",
    "    pl.col(\"pos_tag\").cast(pl.Utf8),\n",
    "    pl.col(\"lemma\").cast(pl.Utf8),\n",
    "    pl.col(\"sentence_id\").cast(pl.Int32),\n",
    "    pl.col(\"token_id_sent\").cast(pl.Int32),\n",
    "])\n",
    "\n",
    "# Save features\n",
    "FEATURES_PARQUET = PROC / \"features_by_word.parquet\"\n",
    "FEATURES_CSV = PROC / \"features_by_word.csv\"\n",
    "\n",
    "features.write_parquet(FEATURES_PARQUET)\n",
    "features.write_csv(FEATURES_CSV)\n",
    "\n",
    "print(f\"Saved features: {features.height} rows -> {FEATURES_PARQUET}\")\n",
    "print(f\"Columns: {features.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6540c5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge with TRT data for modeling\n",
    "print(\"Merging features with TRT data...\")\n",
    "\n",
    "trt_with_features = trt.join(\n",
    "    features, \n",
    "    on=[\"stimulus\", \"index\", \"content\"], \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Save merged data\n",
    "TRT_FEATURES_PARQUET = PROC / \"trt_with_features.parquet\"\n",
    "TRT_FEATURES_CSV = PROC / \"trt_with_features.csv\"\n",
    "\n",
    "trt_with_features.write_parquet(TRT_FEATURES_PARQUET)\n",
    "trt_with_features.write_csv(TRT_FEATURES_CSV)\n",
    "\n",
    "print(f\"Saved merged data: {trt_with_features.height} rows -> {TRT_FEATURES_PARQUET}\")\n",
    "\n",
    "# Basic statistics\n",
    "null_counts = trt_with_features.null_count()\n",
    "print(\"\\nNull counts in merged data:\")\n",
    "for col in [\"freq_zipf\", \"dep_dist\", \"depth\", \"integration_cost\"]:\n",
    "    nulls = null_counts.select(pl.col(col)).item()\n",
    "    total = trt_with_features.height\n",
    "    print(f\"{col}: {nulls}/{total} ({nulls/total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nFinal columns: {trt_with_features.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa4d29",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coverage and validation report\n",
    "print(\"\\n=== ALIGNMENT COVERAGE REPORT ===\")\n",
    "good_coverage = sum(1 for _, cov, _, _ in coverage_stats if cov >= 0.9)\n",
    "print(f\"Stimuli with ≥90% token alignment: {good_coverage}/{len(coverage_stats)}\")\n",
    "\n",
    "print(\"\\nPer-stimulus coverage:\")\n",
    "for stimulus, coverage, total_tokens, aligned_tokens in sorted(coverage_stats, key=lambda x: x[1]):\n",
    "    print(f\"{stimulus}: {coverage:.1%} ({aligned_tokens}/{total_tokens})\")\n",
    "\n",
    "print(\"\\n=== FEATURE SUMMARY ===\")\n",
    "feature_summary = features.select([\n",
    "    pl.col(\"word_len\").mean().alias(\"avg_word_len\"),\n",
    "    pl.col(\"freq_zipf\").mean().alias(\"avg_freq_zipf\"),\n",
    "    pl.col(\"dep_dist\").mean().alias(\"avg_dep_dist\"),\n",
    "    pl.col(\"depth\").mean().alias(\"avg_depth\"),\n",
    "    pl.col(\"integration_cost\").mean().alias(\"avg_integration_cost\"),\n",
    "]).to_pandas().iloc[0]\n",
    "\n",
    "for col, val in feature_summary.items():\n",
    "    print(f\"{col}: {val:.2f}\")\n",
    "\n",
    "print(\"\\n=== DEPENDENCY RELATIONS ===\")\n",
    "dep_counts = features.group_by(\"dep_label\").len().sort(\"len\", descending=True).head(10)\n",
    "print(dep_counts.to_pandas())\n",
    "\n",
    "print(\"\\nPipeline completed successfully!\")\n",
    "print(f\"Ready for mixed-effects modeling with {trt_with_features.height} observations\")\n",
    "print(f\"Subjects: {trt_with_features['subject_id'].n_unique()}\")\n",
    "print(f\"Conditions: {sorted(trt_with_features['condition'].unique().to_list())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0425a6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Mixed-effects pipeline (primary study model)\n",
    "This is the mixed-effects pipeline.\n",
    "- Outcome: log total reading time (TRT) with light trimming.\n",
    "- Predictors: standardized frequency, length, and locality metrics; sum-coded condition.\n",
    "- Random effects: crossed random intercepts (subject + item); optional subject slopes for frequency and length if stable/improving fit.\n",
    "- Exports: fixed effects table, model choice/metrics, residual diagnostics, and simple fit summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8cb8b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports and data load\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE = Path('data-clean') / 'processed'\n",
    "PARQ = BASE / 'trt_with_features.parquet'\n",
    "CSV = BASE / 'trt_with_features.csv'\n",
    "OUT = BASE\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pl.read_parquet(PARQ) if PARQ.exists() else pl.read_csv(CSV)\n",
    "use = (df\n",
    "       .select(['total_reading_time','subject_id','stimulus','condition',\n",
    "                'word_len','freq_zipf','dep_dist','depth','integration_cost'])\n",
    "       .drop_nulls(subset=['total_reading_time','word_len','freq_zipf'])\n",
    ").to_pandas()\n",
    "\n",
    "# Ensure identifiers are strings (safer for formulas)\n",
    "use['subject_id'] = use['subject_id'].astype(str)\n",
    "use['stimulus'] = use['stimulus'].astype(str)\n",
    "\n",
    "# Light trimming and log-transform (literature-aligned)\n",
    "use = use[(use['total_reading_time'] >= 150) & (use['total_reading_time'] <= 4000)].copy()\n",
    "use['log_trt'] = np.log(use['total_reading_time'])\n",
    "\n",
    "# Standardize continuous predictors\n",
    "for c in ['word_len','freq_zipf','dep_dist','depth','integration_cost']:\n",
    "    s = use[c].std()\n",
    "    use[c + '_z'] = (use[c] - use[c].mean()) / (s if s and not np.isnan(s) else 1.0)\n",
    "\n",
    "print('Rows after trimming:', len(use))\n",
    "use.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d174c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit primary model: crossed random intercepts (subject + item)\n",
    "vc = {'stimulus': '0 + C(stimulus)'}\n",
    "formula = 'log_trt ~ freq_zipf_z + word_len_z + dep_dist_z + depth_z + integration_cost_z + C(condition, Sum)'\n",
    "m_crossed = sm.MixedLM.from_formula(formula, groups='subject_id', vc_formula=vc, re_formula='1', data=use)\n",
    "res_crossed = m_crossed.fit(method='lbfgs', reml=True)\n",
    "print('=== MixedLM (crossed intercepts) ===')\n",
    "print(res_crossed.summary())\n",
    "\n",
    "# Attempt subject slopes (freq & length) + item VC; fall back on failure\n",
    "res_slopes = None\n",
    "try:\n",
    "    m_slopes = sm.MixedLM.from_formula(formula, groups='subject_id', vc_formula=vc, re_formula='1 + freq_zipf_z + word_len_z', data=use)\n",
    "    res_slopes = m_slopes.fit(method='lbfgs', reml=True)\n",
    "    print('=== MixedLM (subject slopes: freq, length) + item VC ===')\n",
    "    print(res_slopes.summary())\n",
    "except Exception as e:\n",
    "    print('[Info] Subject-slopes model failed:', repr(e))\n",
    "\n",
    "# Choose model (use ML AIC if available; otherwise slope-variance heuristic)\n",
    "chosen = 'crossed'\n",
    "chosen_res = res_crossed\n",
    "\n",
    "# Try ML AIC selection without altering REML reporting\n",
    "aic_ml = {'crossed': None, 'slopes': None}\n",
    "try:\n",
    "    m_crossed_ml = sm.MixedLM.from_formula(formula, groups='subject_id', vc_formula=vc, re_formula='1', data=use)\n",
    "    res_crossed_ml = m_crossed_ml.fit(method='lbfgs', reml=False)\n",
    "    aic_ml['crossed'] = float(getattr(res_crossed_ml, 'aic', np.nan))\n",
    "    if res_slopes is not None:\n",
    "        m_slopes_ml = sm.MixedLM.from_formula(formula, groups='subject_id', vc_formula=vc, re_formula='1 + freq_zipf_z + word_len_z', data=use)\n",
    "        res_slopes_ml = m_slopes_ml.fit(method='lbfgs', reml=False)\n",
    "        aic_ml['slopes'] = float(getattr(res_slopes_ml, 'aic', np.nan))\n",
    "except Exception as e:\n",
    "    print('[Info] ML AIC selection skipped:', repr(e))\n",
    "\n",
    "prefer = False\n",
    "if res_slopes is not None:\n",
    "    # Prefer slopes if ML AIC improves\n",
    "    if aic_ml['crossed'] is not None and aic_ml['slopes'] is not None and np.isfinite(aic_ml['crossed']) and np.isfinite(aic_ml['slopes']):\n",
    "        prefer = aic_ml['slopes'] < aic_ml['crossed']\n",
    "    # Or if slope variances are clearly > 0\n",
    "    try:\n",
    "        diag = np.diag(res_slopes.cov_re)\n",
    "        slope_variance_positive = (len(diag) >= 3) and ((float(diag[1]) > 1e-4) or (float(diag[2]) > 1e-4))\n",
    "        prefer = prefer or slope_variance_positive\n",
    "    except Exception:\n",
    "        pass\n",
    "    if prefer:\n",
    "        chosen = 'slopes'\n",
    "        chosen_res = res_slopes\n",
    "\n",
    "print(f'Chosen model: {chosen}')\n",
    "\n",
    "# Export fixed effects and decision\n",
    "fe = pd.DataFrame({'coef': chosen_res.params, 'se': chosen_res.bse})\n",
    "fe.to_csv(OUT / 'mixedlm_fixed_effects_chosen.csv')\n",
    "choice = {\n",
    "    'chosen': chosen,\n",
    "    'aic': {\n",
    "        'crossed': float(getattr(res_crossed, 'aic', np.nan)),\n",
    "        'slopes': float(getattr(res_slopes, 'aic', np.nan)) if res_slopes is not None else None\n",
    "    },\n",
    "    'aic_ml': aic_ml,\n",
    "    'llf': {\n",
    "        'crossed': float(res_crossed.llf),\n",
    "        'slopes': float(res_slopes.llf) if res_slopes is not None else None\n",
    "    }\n",
    "}\n",
    "with open(OUT / 'mixedlm_choice.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(choice, f, indent=2)\n",
    "print('Saved: mixedlm_fixed_effects_chosen.csv, mixedlm_choice.json')\n",
    "\n",
    "# Residual diagnostics: fitted vs observed, residual histogram\n",
    "try:\n",
    "    fitted = chosen_res.fittedvalues\n",
    "    resid = use.loc[fitted.index, 'log_trt'] - fitted\n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.scatter(fitted, use.loc[fitted.index, 'log_trt'], s=4, alpha=0.3)\n",
    "    plt.xlabel('Fitted log(TRT)')\n",
    "    plt.ylabel('Observed log(TRT)')\n",
    "    plt.title('Observed vs Fitted (log scale)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT / 'mixedlm_obs_vs_fitted.png', dpi=150)\n",
    "    plt.close()\n",
    "    # Histogram of residuals\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.hist(resid, bins=50, alpha=0.8)\n",
    "    plt.xlabel('Residual (log TRTs)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Residuals histogram')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT / 'mixedlm_residuals_hist.png', dpi=150)\n",
    "    plt.close()\n",
    "    print('Saved: mixedlm_obs_vs_fitted.png, mixedlm_residuals_hist.png')\n",
    "except Exception as e:\n",
    "    print('[Info] Diagnostics plotting skipped:', repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32616141",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Condition effects analysis (adjusted means and contrasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6300db6c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data, mirror preprocessing, and compute condition means/contrasts\n",
    "from pathlib import Path\n",
    "import json, numpy as np, pandas as pd\n",
    "import polars as pl\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrix\n",
    "\n",
    "BASE = Path(\"data-clean\") / \"processed\"\n",
    "PARQ = BASE / \"trt_with_features.parquet\"\n",
    "CSV = BASE / \"trt_with_features.csv\"\n",
    "OUT = BASE\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load and mirror pipeline preprocessing\n",
    "df = pl.read_parquet(PARQ) if PARQ.exists() else pl.read_csv(CSV)\n",
    "use = (df\n",
    "       .select([\"total_reading_time\",\"subject_id\",\"stimulus\",\"condition\",\n",
    "                \"word_len\",\"freq_zipf\",\"dep_dist\",\"depth\",\"integration_cost\"])\n",
    "       .drop_nulls(subset=[\"total_reading_time\",\"word_len\",\"freq_zipf\"])\n",
    "      ).to_pandas()\n",
    "\n",
    "use[\"subject_id\"] = use[\"subject_id\"].astype(str)\n",
    "use[\"stimulus\"]   = use[\"stimulus\"].astype(str)\n",
    "use = use[(use[\"total_reading_time\"] >= 150) & (use[\"total_reading_time\"] <= 4000)].copy()\n",
    "use[\"log_trt\"] = np.log(use[\"total_reading_time\"])\n",
    "for c in [\"word_len\",\"freq_zipf\",\"dep_dist\",\"depth\",\"integration_cost\"]:\n",
    "    s = use[c].std()\n",
    "    use[c + \"_z\"] = (use[c] - use[c].mean()) / (s if s and not np.isnan(s) else 1.0)\n",
    "\n",
    "# Read chosen model and refit to get FE covariance\n",
    "choice_path = OUT / \"mixedlm_choice.json\"\n",
    "chosen = \"crossed\"\n",
    "try:\n",
    "    with choice_path.open() as f:\n",
    "        chosen = json.load(f).get(\"chosen\", \"crossed\")\n",
    "except FileNotFoundError:\n",
    "    print(\"[Info] mixedlm_choice.json not found; defaulting to crossed intercepts\")\n",
    "\n",
    "formula_fe = \"log_trt ~ freq_zipf_z + word_len_z + dep_dist_z + depth_z + integration_cost_z + C(condition, Sum)\"\n",
    "vc = {\"stimulus\": \"0 + C(stimulus)\"}\n",
    "if chosen == \"slopes\":\n",
    "    m = sm.MixedLM.from_formula(formula_fe, groups=\"subject_id\", vc_formula=vc,\n",
    "                                re_formula=\"1 + freq_zipf_z + word_len_z\", data=use)\n",
    "else:\n",
    "    m = sm.MixedLM.from_formula(formula_fe, groups=\"subject_id\", vc_formula=vc,\n",
    "                                re_formula=\"1\", data=use)\n",
    "res = m.fit(method=\"lbfgs\", reml=True)\n",
    "fe = res.fe_params\n",
    "cov_fe = res.cov_params()\n",
    "# Keep only the fixed-effects block to align with X columns\n",
    "if isinstance(cov_fe, np.ndarray):\n",
    "    cov_fe = pd.DataFrame(cov_fe, index=fe.index, columns=fe.index)\n",
    "else:\n",
    "    cov_fe = cov_fe.loc[fe.index, fe.index]\n",
    "\n",
    "# Build design rows per condition at z=0\n",
    "levels = pd.Index(sorted(use[\"condition\"].unique()))\n",
    "tmpl = pd.DataFrame({\n",
    "    \"freq_zipf_z\":[0.0]*len(levels),\n",
    "    \"word_len_z\":[0.0]*len(levels),\n",
    "    \"dep_dist_z\":[0.0]*len(levels),\n",
    "    \"depth_z\":[0.0]*len(levels),\n",
    "    \"integration_cost_z\":[0.0]*len(levels),\n",
    "    \"condition\": levels\n",
    "})\n",
    "X = dmatrix(\"1 + freq_zipf_z + word_len_z + dep_dist_z + depth_z + integration_cost_z + C(condition, Sum)\",\n",
    "            tmpl, return_type=\"dataframe\")\n",
    "X = X.reindex(columns=fe.index, fill_value=0.0)\n",
    "\n",
    "# Marginal means per condition (fixed effects)\n",
    "mu = (X.values @ fe.values)\n",
    "se = np.sqrt(np.einsum(\"ij,jk,ik->i\", X.values, cov_fe.values, X.values))\n",
    "lo, hi = mu - 1.96*se, mu + 1.96*se\n",
    "means = pd.DataFrame({\n",
    "    \"condition\": levels,\n",
    "    \"log_mean\": mu,\n",
    "    \"log_se\": se,\n",
    "    \"log_ci_lo\": lo,\n",
    "    \"log_ci_hi\": hi,\n",
    "})\n",
    "means[\"pct_mean\"]  = (np.exp(means[\"log_mean\"]) - 1.0) * 100.0\n",
    "means[\"pct_ci_lo\"] = (np.exp(means[\"log_ci_lo\"]) - 1.0) * 100.0\n",
    "means[\"pct_ci_hi\"] = (np.exp(means[\"log_ci_hi\"]) - 1.0) * 100.0\n",
    "\n",
    "# Pairwise contrasts\n",
    "rows = []\n",
    "for i in range(len(levels)):\n",
    "    for j in range(i+1, len(levels)):\n",
    "        ci, cj = levels[i], levels[j]\n",
    "        cvec = (X.iloc[i] - X.iloc[j]).values\n",
    "        est = float(cvec @ fe.values)\n",
    "        se  = float(np.sqrt(cvec @ cov_fe.values @ cvec))\n",
    "        lo, hi = est - 1.96*se, est + 1.96*se\n",
    "        rows.append({\n",
    "            \"contrast\": f\"{ci} - {cj}\",\n",
    "            \"log_diff\": est,\n",
    "            \"log_se\": se,\n",
    "            \"log_ci_lo\": lo,\n",
    "            \"log_ci_hi\": hi,\n",
    "            \"pct_diff\": (np.exp(est) - 1.0) * 100.0,\n",
    "            \"pct_ci_lo\": (np.exp(lo) - 1.0) * 100.0,\n",
    "            \"pct_ci_hi\": (np.exp(hi) - 1.0) * 100.0,\n",
    "        })\n",
    "contrasts = pd.DataFrame(rows)\n",
    "\n",
    "# Save\n",
    "means.to_csv(OUT / \"condition_marginal_means.csv\", index=False)\n",
    "contrasts.to_csv(OUT / \"condition_pairwise_contrasts.csv\", index=False)\n",
    "print(\"Saved:\", OUT / \"condition_marginal_means.csv\", \"|\", OUT / \"condition_pairwise_contrasts.csv\")\n",
    "\n",
    "# Small summary\n",
    "print(\"\\nMarginal means (%):\\n\", means[[\"condition\",\"pct_mean\",\"pct_ci_lo\",\"pct_ci_hi\"]].round(2))\n",
    "print(\"\\nPairwise contrasts (%):\\n\", contrasts[[\"contrast\",\"pct_diff\",\"pct_ci_lo\",\"pct_ci_hi\"]].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c9a0fc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Linguistic feature differences across gaze conditions\n",
    "\n",
    "- Describes how features (frequency, length, dependency distance, depth, integration cost) differ by condition (neg/zero/pos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ccd1e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "import math\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Paths\n",
    "BASE = Path('data-clean') / 'processed'\n",
    "PQ = BASE / 'trt_with_features.parquet'\n",
    "CSV = BASE / 'trt_with_features.csv'\n",
    "OUT = BASE\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load\n",
    "df = pl.read_parquet(PQ) if PQ.exists() else pl.read_csv(CSV)\n",
    "use = (\n",
    "    df.select([\n",
    "        'total_reading_time','subject_id','stimulus','condition',\n",
    "        'word_len','freq_zipf','dep_dist','depth','integration_cost'\n",
    "    ])\n",
    "    .drop_nulls(subset=['total_reading_time','word_len','freq_zipf'])\n",
    ").to_pandas()\n",
    "\n",
    "# Trim TRT and transform\n",
    "use = use[(use['total_reading_time'] >= 150) & (use['total_reading_time'] <= 4000)].copy()\n",
    "use['log_trt'] = np.log(use['total_reading_time'])\n",
    "\n",
    "# Standardize features globally\n",
    "FEATURES = ['freq_zipf','word_len','dep_dist','depth','integration_cost']\n",
    "for c in FEATURES:\n",
    "    mu = use[c].mean(); sd = use[c].std()\n",
    "    use[c + '_z'] = (use[c] - mu) / (sd if sd and not np.isnan(sd) else 1.0)\n",
    "\n",
    "# Basic integrity checks\n",
    "conds = sorted(use['condition'].unique().tolist())\n",
    "print('Conditions:', conds)\n",
    "print('Rows:', len(use), '| Subjects:', use['subject_id'].nunique(), '| Stimuli:', use['stimulus'].nunique())\n",
    "assert set(conds) == set(['neg','pos','zero']), 'Expected three conditions: neg,pos,zero'\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4ebf5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Descriptive summaries by condition\n",
    "summ_rows = []\n",
    "for c in FEATURES:\n",
    "    g = use.groupby('condition')[c].agg(['count','mean','std','median'])\n",
    "    g['feature'] = c\n",
    "    g = g.reset_index()[['feature','condition','count','mean','std','median']]\n",
    "    summ_rows.append(g)\n",
    "summ = pd.concat(summ_rows, ignore_index=True)\n",
    "summ.to_csv(OUT / 'feature_condition_descriptives.csv', index=False)\n",
    "print('Saved descriptive stats ->', OUT / 'feature_condition_descriptives.csv')\n",
    "\n",
    "# Plots: boxplots for each feature across conditions\n",
    "for c in FEATURES:\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    data = [use.loc[use['condition']==k, c].dropna().values for k in ['neg','zero','pos']]\n",
    "    ax.boxplot(data, labels=['neg','zero','pos'], showfliers=False)\n",
    "    ax.set_title(f'{c} by condition')\n",
    "    ax.set_ylabel(c)\n",
    "    fig.tight_layout()\n",
    "    p = OUT / f'feature_boxplot_{c}.png'\n",
    "    fig.savefig(p, dpi=150)\n",
    "    plt.close(fig)\n",
    "print('Saved boxplots to', OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa98f65",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mixed-effects: feature_z ~ C(condition, Sum) with subject intercept and stimulus VC\n",
    "vc = {'stimulus': '0 + C(stimulus)'}\n",
    "rows = []\n",
    "for c in FEATURES:\n",
    "    y = c + '_z'\n",
    "    formula = f\"{y} ~ C(condition, Sum)\"\n",
    "    try:\n",
    "        m = sm.MixedLM.from_formula(formula, groups='subject_id', vc_formula=vc, re_formula='1', data=use)\n",
    "        res = m.fit(method='lbfgs', reml=True)\n",
    "        # Extract condition coefficients and CIs\n",
    "        coefs = res.params\n",
    "        ses = res.bse\n",
    "        ci = res.conf_int()\n",
    "        # Save per-feature details\n",
    "        ci_lo_col = 0 if 0 in ci.columns else ci.columns[0]\n",
    "        ci_hi_col = 1 if 1 in ci.columns else ci.columns[1]\n",
    "        df_out = pd.DataFrame({\n",
    "            'term': coefs.index,\n",
    "            'coef': coefs.values,\n",
    "            'se': ses.values,\n",
    "            'ci_lo': ci[ci_lo_col].values,\n",
    "            'ci_hi': ci[ci_hi_col].values\n",
    "        })\n",
    "        df_out.to_csv(OUT / f'feature_mixedlm_{c}.csv', index=False)\n",
    "        # Add to combined summary only condition terms\n",
    "        keep = [t for t in coefs.index if t.startswith('C(condition, Sum)')]\n",
    "        for t in keep:\n",
    "            rows.append({\n",
    "                'feature': c,\n",
    "                'term': t,\n",
    "                'coef': float(coefs[t]),\n",
    "                'se': float(ses[t]),\n",
    "                'ci_lo': float(ci.loc[t, ci_lo_col]),\n",
    "                'ci_hi': float(ci.loc[t, ci_hi_col])\n",
    "            })\n",
    "        print(f'[OK] {c} model fit; saved ->', OUT / f'feature_mixedlm_{c}.csv')\n",
    "    except Exception as e:\n",
    "        print(f'[Warn] {c} model failed:', repr(e))\n",
    "\n",
    "combined = pd.DataFrame(rows)\n",
    "combined.to_csv(OUT / 'feature_condition_mixedlm_summary.csv', index=False)\n",
    "print('Saved combined feature~condition summary ->', OUT / 'feature_condition_mixedlm_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a8b84",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional: TRT condition effects with and without features\n",
    "def pct(b):\n",
    "    return 100.0 * (math.exp(b) - 1.0)\n",
    "\n",
    "try:\n",
    "    # Without features\n",
    "    m0 = sm.MixedLM.from_formula('log_trt ~ C(condition, Sum)', groups='subject_id', vc_formula={'stimulus': '0 + C(stimulus)'}, re_formula='1', data=use).fit(method='lbfgs', reml=True)\n",
    "    coefs0 = m0.params[m0.params.index.str.startswith('C(condition, Sum)')]\n",
    "    out0 = pd.DataFrame({'term': coefs0.index, 'coef': coefs0.values})\n",
    "    out0['pct'] = out0['coef'].apply(pct)\n",
    "    out0.to_csv(OUT / 'trt_condition_effects_without_features.csv', index=False)\n",
    "\n",
    "    # With features\n",
    "    m1 = sm.MixedLM.from_formula('log_trt ~ C(condition, Sum) + freq_zipf_z + word_len_z + dep_dist_z + depth_z + integration_cost_z', groups='subject_id', vc_formula={'stimulus': '0 + C(stimulus)'}, re_formula='1 + freq_zipf_z + word_len_z', data=use).fit(method='lbfgs', reml=True)\n",
    "    coefs1 = m1.params[m1.params.index.str.startswith('C(condition, Sum)')]\n",
    "    out1 = pd.DataFrame({'term': coefs1.index, 'coef': coefs1.values})\n",
    "    out1['pct'] = out1['coef'].apply(pct)\n",
    "    out1.to_csv(OUT / 'trt_condition_effects_with_features.csv', index=False)\n",
    "    print('Saved TRT condition effect tables (with/without features).')\n",
    "except Exception as e:\n",
    "    print('[Info] TRT with/without features block skipped due to error:', repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55af3ac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Done. Outputs are saved under `data-clean/processed/`:\n",
    "- `feature_condition_descriptives.csv`\n",
    "- `feature_boxplot_*.png`\n",
    "- `feature_mixedlm_*.csv` and combined `feature_condition_mixedlm_summary.csv`\n",
    "- `trt_condition_effects_without_features.csv` and `trt_condition_effects_with_features.csv` (if successful)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e1f75b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Reading study — compact report\n",
    "\n",
    "This report summarizes:\n",
    "1) Main: Effects of lexical/syntactic features on log(TRT).\n",
    "2) Secondary 1: Feature differences across conditions.\n",
    "3) Secondary 2: TRT differences across conditions.\n",
    "\n",
    "Figures: (A) Forest plot of feature effects (percent change) and (B) Condition marginal means with 95% CIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa27ce9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE = Path('data-clean') / 'processed'\n",
    "OUT = BASE  # save alongside existing artifacts\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "paths = {\n",
    "  'fe_validation': BASE / 'mixedlm_fe_summary_validation.csv',\n",
    "  'fe_chosen': BASE / 'mixedlm_fixed_effects_chosen.csv',\n",
    "  'cond_means': BASE / 'condition_marginal_means.csv',\n",
    "  'cond_contrasts': BASE / 'condition_pairwise_contrasts.csv',\n",
    "  'feat_desc': BASE / 'feature_condition_descriptives.csv',\n",
    "  'feat_mixed': BASE / 'feature_condition_mixedlm_summary.csv'\n",
    "}\n",
    "for k, p in paths.items():\n",
    "    print(f'{k}:', p.exists(), '->', p)\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e969f0a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Main: Feature effects on log(TRT) — table (percent change with 95% CI)\n",
    "def pct(x):\n",
    "    return 100.0 * (math.exp(x) - 1.0)\n",
    "\n",
    "terms = ['freq_zipf_z','word_len_z','dep_dist_z','depth_z','integration_cost_z']\n",
    "labels = {\n",
    "    'freq_zipf_z':'Frequency (Zipf)',\n",
    "    'word_len_z':'Word length',\n",
    "    'dep_dist_z':'Dependency distance',\n",
    "    'depth_z':'Syntactic depth',\n",
    "    'integration_cost_z':'Integration cost'\n",
    "}\n",
    "\n",
    "# Prefer validation summary (has pct_change columns)\n",
    "if paths['fe_validation'].exists():\n",
    "    fe = pd.read_csv(paths['fe_validation'])\n",
    "    \n",
    "    # Handle case where terms might be in index\n",
    "    if 'term' not in fe.columns and fe.index.name is not None:\n",
    "        fe = fe.reset_index()\n",
    "        if 'index' in fe.columns:\n",
    "            fe = fe.rename(columns={'index': 'term'})\n",
    "    elif 'term' not in fe.columns:\n",
    "        fe = fe.reset_index()\n",
    "        fe = fe.rename(columns={'index': 'term'})\n",
    "    \n",
    "    fe = fe[fe['term'].isin(terms)].copy()\n",
    "    # Expect columns: term, estimate, lo95, hi95, pct_change_est, pct_change_lo, pct_change_hi\n",
    "    required = {'term','estimate','lo95','hi95','pct_change_est','pct_change_lo','pct_change_hi'}\n",
    "    if not required.issubset(fe.columns):\n",
    "        print(f\"Available columns: {fe.columns.tolist()}\")\n",
    "        raise ValueError('mixedlm_fe_summary_validation.csv missing expected columns')\n",
    "    fe['label'] = fe['term'].map(labels)\n",
    "    tbl_main = fe[['label','pct_change_est','pct_change_lo','pct_change_hi','estimate','lo95','hi95']]\\\n",
    "        .rename(columns={\n",
    "            'pct_change_est':'percent',\n",
    "            'pct_change_lo':'percent_lo',\n",
    "            'pct_change_hi':'percent_hi',\n",
    "            'estimate':'beta',\n",
    "            'lo95':'ci_lo',\n",
    "            'hi95':'ci_hi'\n",
    "        }).sort_values('label').reset_index(drop=True)\n",
    "elif paths['fe_chosen'].exists():\n",
    "    # Fallback: compute percent from beta and CI if present\n",
    "    fe = pd.read_csv(paths['fe_chosen'])\n",
    "    \n",
    "    print(f\"Original columns: {fe.columns.tolist()}\")\n",
    "    print(f\"Data shape: {fe.shape}\")\n",
    "    print(f\"First few rows:\\n{fe.head()}\")\n",
    "    \n",
    "    # The CSV has 'Unnamed: 0' containing the term names - rename it to 'term'\n",
    "    if 'Unnamed: 0' in fe.columns:\n",
    "        fe = fe.rename(columns={'Unnamed: 0': 'term'})\n",
    "    \n",
    "    print(f\"After renaming - Available columns: {fe.columns.tolist()}\")\n",
    "    print(f\"Terms available: {fe['term'].tolist()}\")\n",
    "    \n",
    "    # Filter for the terms we want\n",
    "    fe_filtered = fe[fe['term'].isin(terms)].copy()\n",
    "    \n",
    "    print(f\"Filtered data shape: {fe_filtered.shape}\")\n",
    "    print(f\"Filtered terms: {fe_filtered['term'].tolist()}\")\n",
    "    \n",
    "    if len(fe_filtered) == 0:\n",
    "        print(f\"No exact matches found for terms: {terms}\")\n",
    "        print(f\"Available terms: {fe['term'].tolist()}\")\n",
    "        raise ValueError(\"No matching feature terms found in the data\")\n",
    "    \n",
    "    # Add labels\n",
    "    fe_filtered['label'] = fe_filtered['term'].map(labels)\n",
    "    \n",
    "    # Check for coefficient column\n",
    "    coef_col = 'coef' if 'coef' in fe_filtered.columns else 'estimate'\n",
    "    \n",
    "    # Calculate percent changes and confidence intervals\n",
    "    fe_filtered['percent'] = fe_filtered[coef_col].astype(float).apply(pct)\n",
    "    \n",
    "    if 'se' in fe_filtered.columns:\n",
    "        # Calculate CI from standard error\n",
    "        fe_filtered['ci_lo'] = fe_filtered[coef_col].astype(float) - 1.96 * fe_filtered['se'].astype(float)\n",
    "        fe_filtered['ci_hi'] = fe_filtered[coef_col].astype(float) + 1.96 * fe_filtered['se'].astype(float)\n",
    "        fe_filtered['percent_lo'] = fe_filtered['ci_lo'].apply(pct)\n",
    "        fe_filtered['percent_hi'] = fe_filtered['ci_hi'].apply(pct)\n",
    "        tbl_main = fe_filtered[['label','percent','percent_lo','percent_hi',coef_col,'ci_lo','ci_hi']]\\\n",
    "            .rename(columns={coef_col:'beta'})\n",
    "    else:\n",
    "        tbl_main = fe_filtered[['label','percent',coef_col]]\\\n",
    "            .rename(columns={coef_col:'beta'})\n",
    "        \n",
    "    # Sort by label for consistent presentation\n",
    "    tbl_main = tbl_main.sort_values('label').reset_index(drop=True)\n",
    "else:\n",
    "    raise FileNotFoundError('No FE summary file found for main table.')\n",
    "\n",
    "# Save and display\n",
    "tbl_main.to_csv(OUT / 'report_main_feature_effects.csv', index=False)\n",
    "print(f\"Final table shape: {tbl_main.shape}\")\n",
    "print(f\"Final columns: {tbl_main.columns.tolist()}\")\n",
    "tbl_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb60e6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2) Secondary 1: Feature differences across conditions — tables\n",
    "# (a) MixedLM condition effects on standardized features\n",
    "p = paths['feat_mixed']\n",
    "assert p.exists(), f'Missing {p}'\n",
    "feat_mixed = pd.read_csv(p)\n",
    "# Parse condition labels from terms like C(condition, Sum)[S.pos]\n",
    "def term_to_condition(t):\n",
    "    if 'S.pos' in t:\n",
    "        return 'pos'\n",
    "    if 'S.neg' in t:\n",
    "        return 'neg'\n",
    "    return 'zero?'\n",
    "feat_mixed['condition'] = feat_mixed['term'].apply(term_to_condition)\n",
    "feat_mixed = feat_mixed[['feature','condition','coef','ci_lo','ci_hi']].copy()\n",
    "feat_mixed = feat_mixed.sort_values(['feature','condition']).reset_index(drop=True)\n",
    "feat_mixed.to_csv(OUT / 'report_feature_condition_effects.csv', index=False)\n",
    "feat_mixed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806058e8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (b) Descriptive means by condition for each feature\n",
    "p = paths['feat_desc']\n",
    "assert p.exists(), f'Missing {p}'\n",
    "feat_desc = pd.read_csv(p)\n",
    "wide_means = (feat_desc.pivot_table(index='feature', columns='condition', values='mean', aggfunc='first'))\n",
    "if all(c in wide_means.columns for c in ['neg','zero','pos']):\n",
    "    wide_means = wide_means[['neg','zero','pos']]\n",
    "wide_means.to_csv(OUT / 'report_feature_condition_descriptives_wide.csv')\n",
    "wide_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08f93f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3) Secondary 2: TRT differences across conditions — tables\n",
    "p_means = paths['cond_means']\n",
    "p_contr = paths['cond_contrasts']\n",
    "assert p_means.exists(), f'Missing {p_means}'\n",
    "assert p_contr.exists(), f'Missing {p_contr}'\n",
    "cond_means = pd.read_csv(p_means)[['condition','pct_mean','pct_ci_lo','pct_ci_hi']].copy().sort_values('condition')\n",
    "cond_means.to_csv(OUT / 'report_condition_marginal_means.csv', index=False)\n",
    "\n",
    "cond_contrasts = pd.read_csv(p_contr)[['contrast','pct_diff','pct_ci_lo','pct_ci_hi']].copy()\n",
    "cond_contrasts.to_csv(OUT / 'report_condition_pairwise_contrasts.csv', index=False)\n",
    "cond_means, cond_contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2964ec16",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure seaborn is available right before plotting\n",
    "import seaborn as sns  # idempotent import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65330256",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Figure A: Forest plot for main feature effects (percent with 95% CI)\n",
    "fig, ax = plt.subplots(figsize=(6, 3.8))\n",
    "d = tbl_main.copy()\n",
    "d = d.sort_values('label')\n",
    "y = np.arange(len(d))\n",
    "ax.errorbar(d['percent'], y, xerr=[d['percent']-d.get('percent_lo', d['percent']), d.get('percent_hi', d['percent'])-d['percent']], fmt='o', color='black', ecolor='gray', capsize=3)\n",
    "ax.axvline(0, color='red', lw=1, alpha=0.6)\n",
    "ax.set_yticks(y); ax.set_yticklabels(d['label'])\n",
    "ax.set_xlabel('Percent change in TRT per +1 SD (95% CI)')\n",
    "ax.set_title('Feature effects on reading time')\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT / 'report_fig_feature_effects.png', dpi=150)\n",
    "plt.close(fig)\n",
    "print('Saved figure ->', OUT / 'report_fig_feature_effects.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b5642",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Figure B: Condition marginal means with 95% CI (percent scale)\n",
    "cm = cond_means.copy()\n",
    "order = ['neg','zero','pos']\n",
    "cm['condition'] = pd.Categorical(cm['condition'], categories=order, ordered=True)\n",
    "cm = cm.sort_values('condition')\n",
    "fig, ax = plt.subplots(figsize=(5.5, 3.5))\n",
    "ax.bar(cm['condition'], cm['pct_mean'], color=['#4C78A8','#72B7B2','#F58518'])\n",
    "# error bars\n",
    "yerr = np.vstack([cm['pct_mean'] - cm['pct_ci_lo'], cm['pct_ci_hi'] - cm['pct_mean']])\n",
    "ax.errorbar(cm['condition'], cm['pct_mean'], yerr=yerr, fmt='none', ecolor='black', capsize=4)\n",
    "ax.set_ylabel('Adjusted TRT (percent scale)')\n",
    "ax.set_title('Condition marginal means (95% CI)')\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT / 'report_fig_condition_means.png', dpi=150)\n",
    "plt.close(fig)  \n",
    "print('Saved figure ->', OUT / 'report_fig_condition_means.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3691743b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Outputs written under `data-clean/processed/`:\n",
    "- Tables: `report_main_feature_effects.csv`, `report_feature_condition_effects.csv`, `report_feature_condition_descriptives_wide.csv`,\n",
    "  `report_condition_marginal_means.csv`, `report_condition_pairwise_contrasts.csv`\n",
    "- Figures: `report_fig_feature_effects.png`, `report_fig_condition_means.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2768851c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Figures\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae4e64",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Figure 1 — Which features matter most for reading time\n",
    "This bar chart ranks the linguistic features by how much they change TRT (percent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972606ad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Linguistic feature influence on TRT\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "proc = Path('data-clean/processed')\n",
    "saved = proc / 'report_fig_feature_effects.png'\n",
    "main_csv = proc / 'report_main_feature_effects.csv'\n",
    "\n",
    "if main_csv.exists():\n",
    "    ef = pd.read_csv(main_csv)\n",
    "    # Drop Integration cost rows\n",
    "    ef = ef[~ef['label'].str.contains('integration', case=False, na=False)].copy()\n",
    "    # Use the signed percent column\n",
    "    if 'percent' not in ef.columns:\n",
    "        print('Unexpected columns in feature effects table:', ef.columns.tolist())\n",
    "        display(ef.head())\n",
    "    else:\n",
    "        ef = ef.sort_values(by='percent', key=lambda s: s.abs())\n",
    "        plt.figure(figsize=(9,6))\n",
    "        ax = sns.barplot(data=ef, x='percent', y='label', orient='h', palette='viridis')\n",
    "        for i, row in ef.reset_index(drop=True).iterrows():\n",
    "            sign = '+' if row['percent'] >= 0 else ''\n",
    "            ax.text(row['percent'] + (0.3 if row['percent'] >= 0 else -0.3), i,\n",
    "                    f\"{sign}{row['percent']:.1f}%\", va='center', ha='left' if row['percent'] >= 0 else 'right', color='black')\n",
    "        ax.set_xlabel('Percent change in TRT')\n",
    "        ax.set_ylabel('Feature')\n",
    "        ax.set_title('Linguistic feature influence on TRT')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "elif saved.exists():\n",
    "    display(Image(filename=str(saved)))\n",
    "else:\n",
    "    print('Missing inputs for feature influence figure:', main_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01c6e4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Figure 2 — Feature means by condition \n",
    "Average feature values within each condition (neg, zero, pos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264852b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature influence ranking on TRT\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if we have the main feature effects table\n",
    "main_csv = OUT / 'report_main_feature_effects.csv'\n",
    "rank_path = OUT / 'report_fig_feature_influence_ranking.png'\n",
    "\n",
    "if main_csv.exists():\n",
    "    # Load the feature effects data\n",
    "    ef = pd.read_csv(main_csv)\n",
    "    \n",
    "    # Filter out integration cost\n",
    "    ef = ef[~ef['label'].str.contains('Integration cost', case=False, na=False)]\n",
    "    \n",
    "    print(f\"Feature effects data shape (after filtering): {ef.shape}\")\n",
    "    print(f\"Columns: {ef.columns.tolist()}\")\n",
    "    print(f\"Data:\\n{ef}\")\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Sort by absolute percent change for ranking\n",
    "    ef_sorted = ef.copy()\n",
    "    ef_sorted['abs_percent'] = ef_sorted['percent'].abs()\n",
    "    ef_sorted = ef_sorted.sort_values('abs_percent', ascending=True)\n",
    "    \n",
    "    # Create horizontal bar plot\n",
    "    colors = ['red' if x < 0 else 'blue' for x in ef_sorted['percent']]\n",
    "    bars = plt.barh(range(len(ef_sorted)), ef_sorted['percent'], color=colors, alpha=0.7)\n",
    "    \n",
    "    # Add error bars if confidence intervals are available\n",
    "    if 'percent_lo' in ef_sorted.columns and 'percent_hi' in ef_sorted.columns:\n",
    "        yerr_lo = ef_sorted['percent'] - ef_sorted['percent_lo']\n",
    "        yerr_hi = ef_sorted['percent_hi'] - ef_sorted['percent']\n",
    "        plt.errorbar(ef_sorted['percent'], range(len(ef_sorted)), \n",
    "                    xerr=[yerr_lo, yerr_hi], fmt='none', color='black', capsize=3)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.yticks(range(len(ef_sorted)), ef_sorted['label'])\n",
    "    plt.xlabel('Percent change in Total Reading Time (%)')\n",
    "    plt.title('Feature Influence Ranking on Total Reading Time (Integration Cost Excluded)')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, ef_sorted['percent'])):\n",
    "        plt.text(val + (0.5 if val > 0 else -0.5), i, f'{val:.1f}%', \n",
    "                ha='left' if val > 0 else 'right', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(rank_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved feature influence ranking to: {rank_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "elif rank_path.exists():\n",
    "    # Fallback to existing image\n",
    "    from IPython.display import display, Image\n",
    "    display(Image(filename=str(rank_path)))\n",
    "else:\n",
    "    print(f'Missing both data file ({main_csv}) and image file ({rank_path})')\n",
    "    print(\"Please ensure the main feature effects table has been generated in the previous cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee61aa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Figure 3 — Total reading time by condition\n",
    "We compare neg, zero, pos (in that order) and label the values so the difference is easy to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e86e5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Total reading time by condition (neg → zero → pos)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "proc = Path('data-clean/processed')\n",
    "cm_csv = proc / 'report_condition_marginal_means.csv'\n",
    "cm_png = proc / 'report_fig_condition_means.png'\n",
    "\n",
    "if cm_csv.exists():\n",
    "    cm = pd.read_csv(cm_csv)\n",
    "    order = ['neg','zero','pos']\n",
    "    # Map expected columns\n",
    "    if 'pct' in cm.columns:\n",
    "        ycol = 'pct'; lo_col = 'lo'; hi_col = 'hi'; units = '%'\n",
    "    elif 'pct_mean' in cm.columns and 'pct_ci_lo' in cm.columns and 'pct_ci_hi' in cm.columns:\n",
    "        ycol = 'pct_mean'; lo_col = 'pct_ci_lo'; hi_col = 'pct_ci_hi'; units = '%'\n",
    "    elif 'mean' in cm.columns and 'lo' in cm.columns and 'hi' in cm.columns:\n",
    "        ycol = 'mean'; lo_col = 'lo'; hi_col = 'hi'; units = 'ms'\n",
    "    else:\n",
    "        print('Unexpected columns in condition means table:', cm.columns.tolist())\n",
    "        display(cm.head())\n",
    "        raise SystemExit()\n",
    "\n",
    "    # Ensure correct ordering\n",
    "    cm = cm.set_index('condition').loc[order].reset_index()\n",
    "\n",
    "    plt.figure(figsize=(7,4))\n",
    "    ax = sns.pointplot(data=cm, x='condition', y=ycol, order=order, join=False, color='#2a5', capsize=0.2)\n",
    "\n",
    "    # Error bars if present\n",
    "    if lo_col in cm.columns and hi_col in cm.columns:\n",
    "        for i, row in cm.iterrows():\n",
    "            ax.vlines(i, row[lo_col], row[hi_col], colors='black', lw=1)\n",
    "\n",
    "    # Place numeric labels to the left of each marker for readability\n",
    "    vmin, vmax = cm[ycol].min(), cm[ycol].max()\n",
    "    span = max(1e-9, vmax - vmin)\n",
    "    for i, row in cm.iterrows():\n",
    "        val = row[ycol]\n",
    "        if units == '%':\n",
    "            label = f\"{val:.1f}%\"\n",
    "        else:\n",
    "            label = f\"{val:.0f} ms\"\n",
    "        ax.text(i - 0.12, val, label, ha='right', va='center', fontsize=10, color='black')\n",
    "\n",
    "    ax.set_ylim(vmin - 0.1*span, vmax + 0.1*span)\n",
    "    ax.set_title('Total reading time by condition')\n",
    "    ax.set_xlabel('Condition (neg → zero → pos)')\n",
    "    ax.set_ylabel('Percent change in TRT' if units=='%' else 'TRT (ms)')\n",
    "    ax.grid(axis='y', alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif cm_png.exists():\n",
    "    display(Image(filename=str(cm_png)))\n",
    "else:\n",
    "    print('Missing inputs for condition means figure:', cm_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c98b95",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Figure 4 — Which linguistic features explain TRT\n",
    "We show each feature’s modeled influence on total reading time (percent change), excluding Integration cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7c052",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature means by condition heatmap\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "proc = PROC\n",
    "ordered_png = proc / 'report_fig_feature_means_heatmap_ordered.png'\n",
    "descriptives_path = proc / 'feature_condition_descriptives.csv'\n",
    "wide_path = proc / 'report_feature_condition_descriptives_wide.csv'\n",
    "\n",
    "# Try to generate the heatmap from available data\n",
    "fig_generated = False\n",
    "\n",
    "# First try: use the wide format if available\n",
    "if wide_path.exists():\n",
    "    print(f\"Loading wide format data from: {wide_path}\")\n",
    "    wide = pd.read_csv(wide_path)\n",
    "    print(f\"Wide data columns: {wide.columns.tolist()}\")\n",
    "    print(f\"Wide data shape: {wide.shape}\")\n",
    "    print(f\"First few rows:\\n{wide.head()}\")\n",
    "    \n",
    "    # Check for condition columns\n",
    "    condition_cols = [c for c in wide.columns if c in {'neg','zero','pos'}]\n",
    "    if len(condition_cols) >= 2 and 'feature' in wide.columns:\n",
    "        # Filter out integration cost if present\n",
    "        df = wide[['feature'] + condition_cols].copy()\n",
    "        df = df[~df['feature'].str.contains('integration', case=False, na=False)]\n",
    "        \n",
    "        # Create pivot table for heatmap\n",
    "        df_plot = df.set_index('feature')\n",
    "        order = [c for c in ['neg','zero','pos'] if c in condition_cols]\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(df_plot[order], annot=True, fmt='.3f', cmap='RdBu_r', \n",
    "                   center=df_plot.values.mean(), cbar_kws={'label': 'Feature Value'})\n",
    "        plt.title('Feature Means by Condition (Integration Cost Excluded)')\n",
    "        plt.ylabel('Linguistic Feature')\n",
    "        plt.xlabel('Condition (neg → zero → pos)')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(ordered_png, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved heatmap to: {ordered_png}\")\n",
    "        \n",
    "        plt.show()\n",
    "        fig_generated = True\n",
    "\n",
    "# Second try: use the long format descriptives\n",
    "elif descriptives_path.exists():\n",
    "    print(f\"Loading descriptives data from: {descriptives_path}\")\n",
    "    desc = pd.read_csv(descriptives_path)\n",
    "    print(f\"Descriptives columns: {desc.columns.tolist()}\")\n",
    "    print(f\"Descriptives shape: {desc.shape}\")\n",
    "    print(f\"First few rows:\\n{desc.head()}\")\n",
    "    \n",
    "    # Check if we can create a pivot table\n",
    "    if {'feature', 'condition', 'mean'}.issubset(desc.columns):\n",
    "        # Filter out integration cost if present\n",
    "        desc_filtered = desc[~desc['feature'].str.contains('integration', case=False, na=False)]\n",
    "        \n",
    "        # Create pivot table\n",
    "        heatmap_data = desc_filtered.pivot(index='feature', columns='condition', values='mean')\n",
    "        \n",
    "        # Ensure proper column ordering\n",
    "        order = [c for c in ['neg','zero','pos'] if c in heatmap_data.columns]\n",
    "        if len(order) >= 2:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(heatmap_data[order], annot=True, fmt='.3f', cmap='RdBu_r', \n",
    "                       center=heatmap_data.values.mean(), cbar_kws={'label': 'Feature Value'})\n",
    "            plt.title('Feature Means by Condition (Integration Cost Excluded)')\n",
    "            plt.ylabel('Linguistic Feature')\n",
    "            plt.xlabel('Condition (neg → zero → pos)')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the figure\n",
    "            plt.savefig(ordered_png, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved heatmap to: {ordered_png}\")\n",
    "            \n",
    "            plt.show()\n",
    "            fig_generated = True\n",
    "\n",
    "# Third try: generate from available feature data if we have it\n",
    "if not fig_generated and 'feat_desc' in globals():\n",
    "    print(\"Using feat_desc variable from previous processing\")\n",
    "    print(f\"feat_desc columns: {feat_desc.columns.tolist()}\")\n",
    "    print(f\"feat_desc shape: {feat_desc.shape}\")\n",
    "    \n",
    "    if {'feature', 'condition', 'mean'}.issubset(feat_desc.columns):\n",
    "        # Filter out integration cost if present\n",
    "        desc_filtered = feat_desc[~feat_desc['feature'].str.contains('integration', case=False, na=False)]\n",
    "        \n",
    "        # Create pivot table\n",
    "        heatmap_data = desc_filtered.pivot(index='feature', columns='condition', values='mean')\n",
    "        \n",
    "        # Ensure proper column ordering\n",
    "        order = [c for c in ['neg','zero','pos'] if c in heatmap_data.columns]\n",
    "        if len(order) >= 2:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(heatmap_data[order], annot=True, fmt='.3f', cmap='RdBu_r', \n",
    "                       center=heatmap_data.values.mean(), cbar_kws={'label': 'Feature Value'})\n",
    "            plt.title('Feature Means by Condition (Integration Cost Excluded)')\n",
    "            plt.ylabel('Linguistic Feature')\n",
    "            plt.xlabel('Condition (neg → zero → pos)')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the figure\n",
    "            plt.savefig(ordered_png, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved heatmap to: {ordered_png}\")\n",
    "            \n",
    "            plt.show()\n",
    "            fig_generated = True\n",
    "\n",
    "# Fallback: try to display existing images\n",
    "if not fig_generated:\n",
    "    from IPython.display import display, Image\n",
    "    \n",
    "    if ordered_png.exists():\n",
    "        print(\"Displaying existing ordered heatmap\")\n",
    "        display(Image(filename=str(ordered_png)))\n",
    "    else:\n",
    "        default_png = proc / 'report_fig_feature_means_heatmap.png'\n",
    "        if default_png.exists():\n",
    "            print(\"Displaying existing default heatmap\")\n",
    "            display(Image(filename=str(default_png)))\n",
    "        else:\n",
    "            print(f'Could not generate or find heatmap. Checked paths:')\n",
    "            print(f'- Wide format: {wide_path} (exists: {wide_path.exists()})')\n",
    "            print(f'- Descriptives: {descriptives_path} (exists: {descriptives_path.exists()})')\n",
    "            print(f'- Ordered PNG: {ordered_png} (exists: {ordered_png.exists()})')\n",
    "            print(f'- Default PNG: {default_png} (exists: {default_png.exists()})')\n",
    "            \n",
    "            # Show available files in the processed directory\n",
    "            if proc.exists():\n",
    "                print(f\"\\nAvailable files in {proc}:\")\n",
    "                for f in sorted(proc.glob(\"*feature*\")):\n",
    "                    print(f\"  {f.name}\")\n",
    "            else:\n",
    "                print(f\"Processed directory {proc} does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a8318e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Discussion and conclusion\n",
    "\n",
    "As in the previous studies’ results, frequency showed a negative effect while word length showed a positive effect for TRT, aligning with prior work on eye movements in reading (Rayner, 1998; Kliegl, Nuthmann, & Engbert, 2006). Syntactic depth also offers a positive effect, which is in the same direction as the previous findings (Gibson, 2000). However, dependency distances show a negative effect, which doesn’t completely align with the literature findings (Gibson, 2000). This change might arise from a problem with the pipeline. However, it could also be due to the text's nature of being generated. It might be that language models might utilize dependency distances better than human-generated texts, yet without further research, this can only stay as a hypothesis since it still remains not thoroughly tested in this study.\n",
    "As expected from previous findings, lexical features affect TRT considerably more than syntactical features (Rayner, 1998; Kliegl et al., 2006). Word length and then the frequency are shown to be the strongest features, while syntactic depth and dependency distance follow them, with syntactic depth offering a slight advantage over dependency distance.\n",
    "Surprisingly, in the total reading time by condition ranking, zero has the first place with the lowest score, which is followed by the negative with only a small difference. The last place is positive, as would have been expected, since it aims for longer reading time. The results of the zero might indicate that while generating texts, language models may already aim to create texts that are easy to read- even without the influence of a gaze model or a specific prompt. Therefore, having an even better score than the negative condition texts that were specifically designed to be easier to read.\n",
    "The effects of the linguistic features on gaze conditions show a nearly stable distribution among the conditions, except for the dependency distance feature. The contradictory results of the dependency distance feature might indicate a problem in the pipeline. Other than that dependency, for other features, the distribution of feature effects is either followed as negative to zero to positive, or from positive to zero to negative. This might imply the systematic structures of each text type/condition. Also, as expected, and proved by the earlier studies, the negative condition, which aims for faster reading, proves to have the highest word frequency and lower word length compared to the other two conditions (Rayner, 1998; Kliegl et al., 2006). \n",
    "Surprisingly, although a higher syntactic depth score should indicate longer reading (Gibson, 2000), the results seem to show the opposite. This might, of course, indicate an error in the pipeline; however, it might also display a characteristic of a generated text. Due to its abilities, a model might utilize the longer syntactic depth better than a human; therefore, it might utilize sentences with lower syntactic depth to disrupt the flow of reading while utilizing higher syntactic level sentences to not interrupt the reading with stops or transitions to new sentences. Yet again, with the data at hand, this cannot be proven without further investigation, and it might also be mostly influenced by punctuation or other features. Nevertheless, from the data we have at hand, we can claim that lexical features affect the TRT more than syntactical features.\n",
    "\n",
    "Overall, our results show that lexical features affect total reading time more than syntactical features. It confirms the hypothesis that higher frequency levels will lead to shorter total reading time, while, on the other hand, longer words will lead to longer total reading time. Although it was able to prove that greater syntactic depth will lead to longer total reading time, it fails to find the evidence to prove that longer dependencies lead to longer reading time, as the feature offers a negative score. For the gaze conditions, it successfully shows that the positive gaze condition leads to features that increase the reading time, while the negative gaze condition leads to a decrease in the reading time. However, while the lexical features show a correspondence with the expected influence on the gaze conditions, syntactic features show the opposite effect than expected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de849368",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# AI usage declaration\n",
    "\n",
    "I used the following tools:\n",
    "\n",
    "| Tool           | Purpose                                              |\n",
    "| -------------- | -----------------------------------------------------|\n",
    "| ChatGPT        | Brainstorming research questions & literature review |\n",
    "| GitHub Copilot | Assistance with plotting code                        |\n",
    "| Grammarly      | Grammar checking                                     |\n",
    "\n",
    "\n",
    "- I thoroughly checked all output generated by these tools.\n",
    "- I know that I am responsible for the correctness of all code and content in this report.\n",
    "- I confirm that (apart from the resources listed above) the code and content in this report is my own work.\n",
    "\n",
    "**Signature:** Mert Yeşilyurt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c16bd3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "Baayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. *Journal of Memory and Language, 59*(4), 390–412. \n",
    "\n",
    "Chomsky, N. (1957). *Syntactic structures*. The Hague, Netherlands: Mouton.\n",
    "\n",
    "Fodor, J. A. (1983). *The modularity of mind*. Cambridge, MA: MIT Press.\n",
    "\n",
    "Gibson, E. (2000). The dependency locality theory: A distance-based theory of linguistic complexity. In Y. Miyashita, A. Marantz, & W. O’Neil (Eds.), *Image, language, brain* (pp. 95–126). Cambridge, MA: MIT Press.\n",
    "\n",
    "Grodner, D., & Gibson, E. (2005). Consequences of the serial nature of linguistic input for sentential complexity. *Cognitive Science, 29*(2), 261–290. \n",
    "\n",
    "Kliegl, R., Nuthmann, A., & Engbert, R. (2006). Tracking the mind during reading: The influence of past, present, and future words on fixation durations. *Journal of Experimental Psychology: General, 135*(1), 12–35. \n",
    "\n",
    "Rayner, K. (1998). Eye movements in reading and information processing: 20 years of research. *Psychological Bulletin, 124*(3), 372–422."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.579171,
   "end_time": "2025-08-21T11:04:20.066006",
   "environment_variables": {},
   "exception": true,
   "input_path": "MertYesilyurt_Report.ipynb",
   "output_path": "MertYesilyurt_Report.out.ipynb",
   "parameters": {},
   "start_time": "2025-08-21T11:04:14.486835",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}